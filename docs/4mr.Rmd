# MapReduce

## Introduction to MapReduce

MapReduce is a simple but powerful programming model for breaking a task into pieces and operating on those pieces in an embarrassingly parallel manner across a cluster.  The approach was popularized by Google (Dean & Ghemawat, 2008).

MapReduce forms the basis of all datadr operations.  While the goal of datadr is for the higher-level `r rdl("divide()")` and `r rdl("recombine()")` methods to take care of all analysis needs, there may be times that the user would like to write MapReduce code directly.  datadr exposes general MapReduce interface that runs over any implemented backend.  The most popular of these, of course, is RHIPE.

### MapReduce overview

MapReduce operates on key-value pairs.  The input, output, and intermediate data are all key-value pairs.  A MapReduce job consists of three phases that operate on these key-value pairs: the *map*, the *shuffle/sort*, and the *reduce*:

- **Map**: A map function is applied to each input key-value pair, which does some user-defined processing and emits new key-value pairs to intermediate storage to be processed by the reduce.
- **Shuffle/Sort**: The map output values are collected for each unique map output key and passed to a reduce function.
- **Reduce**: A reduce function is applied in parallel to all values corresponding to each unique map output key and emits output key-value pairs.

A simple schematic of this is shown below.

<img src="image/mroverview.svg" width="450px" alt="mroverview" style="display:block; margin:auto"/>
<!-- ![mroverview](image/mroverview.png) -->

The map function and reduce function are user-defined.  The MapReduce engine takes care of everything else.  We will get a better feel for how things work by looking at some examples in this section.

### Iris data (again)

We will illustrate MapReduce by continuing to look at the iris data.  This time, we'll split it randomly into 4 key-value pairs:

```{r iris_rkv, message=FALSE}
# split iris data randomly into 4 key-value pairs
set.seed(1234)
irisRR <- divide(iris, by = rrDiv(nrows = 40))
```

All inputs and outputs to MapReduce jobs in datadr are ddo or ddf objects.

## MapReduce with datadr

MapReduce jobs are executed in datadr with a call to `r rdl("mrExec()")`.  The main inputs a user should be concerned with are:

- `data`: a ddo/ddf
- `map`: an R expression that is evaluated during the map stage
- `reduce`: a vector of R expressions with names `pre`, `reduce`, and `post` that is evaluated during the reduce stage

Other inputs of interest are the following:

- `setup`:	an expression of R code to be run before `map` and `reduce`
- `output`: a "kvConnection" object indicating where the output data should reside -- see [Store/Compute Backends](#backend-choices)
- `control`: parameters specifying how the backend should handle things (most-likely parameters to rhwatch in RHIPE) -- see [Store/Compute Backends](#backend-choices)
- `params`: a named list of parameters external to the input data that are needed in the map or reduce phases

In datadr, the `map` and `reduce` must be specified by the user as an R expression.

### The `map` expression

The map expression is simply an R expression that operates on a chunk of input key-value pairs.  Map expressions operate in parallel on disjoint chunks of the input data.  For example, if there are 1000 input key-value pairs of roughly equal size and there are 5 map tasks running, then each map task will operate on around 200 key-value pairs over the course of the job.  Depending on the size of each key-value pair, typically a map task will operate on batches of these key-value pairs, say 10 at a time, until all 200 have been processed.

A datadr map expression has the following essential objects or functions available:

- `map.keys`: a list of the current block of input keys
- `map.values`: a list of the current block of input values
- `collect()`: a function that emits key-value pairs to the shuffle/sort process

A map expression skeleton would look like this:

```{r map_exp_example, purl=FALSE}
map <- expression({
  # do some operations on map.keys and map.values
  # emit result to shuffle/sort using collect(key, value)
})
```

A key element of the map expression is the `collect()` function, which passes modified key-value pairs to the shuffle/sort phase prior to the reduce. The first argument of the function is a key, and the second is a value. When you have reached a point in your map expression that you are ready to pass the current processed key-value pair to the reducer, you call `collect()`.

### The `reduce` expression

The reduce expression is processed for each set of unique keys emitted from the running the map expression over the data.  It consists of `pre`, `reduce` and `post` expressions.

A datadr reduce expression has the following essential objects or functions available:

- `reduce.key`: a unique map output key
- `reduce.values`: a collection of all of the map output keys the correspond to `reduce.key`
- `collect()`: a function that emits key-value pairs to the output dataset

For example, say we have a map that emitted key-value pairs: `("a", 1)`, `("a", 2)`, and `("a", 3)`.  The shuffle/sort gathers all map outputs with key `"a"` and sets `reduce.key = "a"` and `reduce.values = list(1, 2, 3)`.

Note that in many cases, there are a very large number of `reduce.values` such that we must iterate through batches of them.  This is the purpose of the `pre`, `reduce`, and `post` parts of the reduce expression.  In the `pre`, we might initialize a result object.  Then the `reduce` part might get called multiple times until all `reduce.values` have been passed through.  Finally, we can post-process the result object and emit it to the output data in the `post` part of the expression.  (Note that we can emit output at any place in the reduce expression, but this is typically how it is done.)

A reduce expression skeleton would look like this:

```{r reduce_exp_example, purl=FALSE}
reduce <- expression(
  pre = {
    # initialize objects in which results will be stored
  },
  reduce = {
    # take current batch of reduce.values and update the result
  },
  post = {
    # emit output key-value pairs using collect(key, value)
  }
)
```

We will now solidify how these are used with some examples.

## MapReduce Examples

<!--
k <- irisDdf[[1]][[1]]
v <- irisDdf[[1]][[2]]
-->

<!--
lapply(irisKV, function(x) max(x[[2]]$Petal.Length))
map.values <- lapply(irisKV[3:4], "[[", 2)
v <- do.call(rbind, map.values)
tmp <- by(v, v$Species, function(x) {
  curSpecies <- as.character(x$Species[1])
  data.frame(tot=sum(x$Petal.Length), n=nrow(x))
})
-->

The easiest way to illustrate MapReduce is through example.  Given the `irisRR` data we just created, let's try a couple of computations:

- Compute the global maximum `Petal.Length`
- Compute the mean `Petal.Length` by species

### Global maximum `Petal.Length`

Recall that `irisRR` is a random partitioning of the iris data, split into 4 key-value pairs.  To compute the global maximum petal length, we simply need to compute the maximum petal length for each key-value pair in the map and then combine these maximums in the reduce and take the max of maxes.  To ensure that all of our maximum values computed in the map go to the same reduce task, we need to emit the same key each time we `collect()`.  We emit the key `"max"` each time.  This will ensure that even across multiple map processes, all results with emitted key `"max"` will be shuffled into the same reduce task, which will have `reduce.key = "max"`.  We write the map as follows:

```{r max_map}
# map expression to emit max petal length for each k/v pair
maxMap <- expression({
  for(curMapVal in map.values)
    collect("max", max(curMapVal$Petal.Length))
})
```

The `map.keys` and `map.values` lists for the current block of input data being processed are available inside the map.  We don't care about the input keys in this case.  We step through `map.values` and emit the maximum petal length for each map value.

Then in the reduce, we set up the variable `globalMax` which we will update as new maximum values arrive.  In the `reduce` part of the expression, we concatenate the current value of `globalMax` to the new batch of `reduce.values` and compute the maximum of that - thus computing the maximum of maximums.  When all `reduce.values` have been processed, we call `collect()` to emit the `reduce.key` (`"max"`), and the computed global maximum.

```{r max_reduce}
# reduce expression to compute global max petal length
maxReduce <- expression(
  pre = {
    globalMax <- NULL
  },
  reduce = {
    globalMax <- max(c(globalMax, unlist(reduce.values)))
  },
  post = {
    collect(reduce.key, globalMax)
  }
)
```

We can execute the job with the following:

```{r max_exec, message=FALSE}
# execute the job
maxRes <- mrExec(irisRR,
  map = maxMap,
  reduce = maxReduce
)
```

The output of `r rdl("mrExec")` is a ddo.  Since we only output one key-value pair, and the key is `"globalMax"`, we can get the result with:

```{r max_result}
# look at the result
maxRes[["max"]]
```

To go through what happened in this job in more detail, here is a visual depiction of what happened:

<img src="image/mr1.svg" width="650px" alt="mr1" style="display:block; margin:auto"/>
<!-- ![mr1](image/mr1.png) -->

In this diagram, we illustrate how the MapReduce would be carried out if there are two map tasks running.  The key-value pairs with keys `"1"` and `"2"` get sent to one map task, and the other two key-value pairs get sent to the other map task.  The first map has available to compute on the objects `map.keys = list("1", "2")` and `map.values`, a list of the values corresponding to keys `"1"` and `"2"`.  In our map expression, we iterate through each of the two `map.value`s and emit key-value pairs shown after the map in the diagram.  This is done for both map tasks.  Then the shuffle/sort groups the data by map output key.  In this case, all map outputs have the same key, so they all get grouped together to be sent to one reduce.  If there are several reduce tasks running, in this case there will only be one doing any work, since there is only one unique map output key.  In the reduce, we have `reduce.key = "max"` and a list `reduce.values = list(6.9, 5.8, 6.7, 6.4)` (note that with different reduce buffer settings, it could be that we first operate on `reduce.values = list(6.9, 5.8)` and then update the result with `reduce.values = list(6.7, 6.4)`).  The reduce expression is applied to the data, and the final output is emitted, the global maximum.

We will look at a slightly more involved example next.

First, note that there are several ways to get to the desired result.  Another way we could have written the map would be to take advantage of having several `map.keys` and `map.values` in a given running map task.  We can compute the max of the maximum of each individual subset, and then only emit one key-value pair per map task:

```{r max_map2}
# another map expression to emit max petal length
maxMap2 <- expression(
  collect(
    "max",
    max(sapply(map.values, function(x) max(x$Petal.Length))))
)
```

With this, we are emitting less data to the reduce.  Typically intermediate data is written to disk and then read back by the reduce, so it is usually a good idea to send as little data to the reduce as possible.

### Mean `Petal.Length` by species

Now we look at an example that shows a little more of a shuffle/sort and also illustrates how a simple summary statistic, the mean, can be broken into independent operations.

Suppose we would like to compute the mean petal length by species.  Computing a mean with independent operations for each subset can be done quite simply by keeping track of the sum and the length of the variable of interest in each subset, adding these up, and then dividing the final sum by the final length (note that this is not numerically stable if we are dealing with a lot of values -- see [here](http://www.janinebennett.org/index_files/ParallelStatisticsAlgorithms.pdf) for a good reference -- these are used in the summary statistics computations for `r rdl("updateAttributes()")`).

So computing the mean in MapReduce is easy.  But we want to compute the mean individually for each species.  We can take care of that in our map expression by breaking the data up by species, and then computing the sum and length for each and emitting them to the reduce using `collect()`.  Remember that you can call `collect()` as many times as you would like, with whatever keys and values you would like.  Here we will choose the map output keys to be the species name, to help get data to the right reduce task.

```{r mean_map}
# map expression to emit sum and length of Petal.Length by species
meanMap <- expression({
  v <- do.call(rbind, map.values)
  tmp <- by(v, v$Species, function(x) {
    collect(
      as.character(x$Species[1]),
      cbind(tot = sum(x$Petal.Length), n = nrow(x)))
  })
})
```

In this map expression, we first bind the `map.values` data frames into one data frame.  Then we call `by` to apply a function to the data frame by species, where for each subset we emit the species and the corresponding sum and length.

For the reduce for each unique map output key, we initialize a value `total = 0` and a length `nn = 0`.  Then, the `reduce` part of the expression is run on all incoming `reduce.values` and `total` and `nn` are updated with the new data.  When we have cycled through all `reduce.values`, we compute the mean as `total / nn` and emit the result:

```{r mean_reduce}
# reduce to compute mean Petal.Length
meanReduce <- expression(
  pre = {
    total <- 0
    nn <- 0
  },
  reduce = {
    tmp <- do.call(rbind, reduce.values)
    total <- total + sum(tmp[, "tot"])
    nn <- nn + sum(tmp[, "n"])
  },
  post = {
    collect(reduce.key, total / nn)
  }
)
```

The job is executed with:

```{r mean_exec, message=FALSE}
# execute the job
meanRes <- mrExec(irisRR,
  map = meanMap,
  reduce = meanReduce
)
```

And we can look at the result:

```{r mean_res}
# look at the result for virginica and versicolor
meanRes[c("virginica", "versicolor")]
```

And now we illustrate what happened in this job:

<img src="image/mr2.svg" width="650px" alt="mr2" style="display:block; margin:auto"/>
<!-- ![mr2](image/mr2.png) -->

We assume the same setup of key-value pairs being sent to two map tasks as before in the global max example.  Each map task takes its input values and `rbind`s them into a single data frame.  Then for each species subset, the species is output as the key and the sum and length are output as the value.  We see that each map task outputs data for each species.  Then the shuffle/sort takes all output with key "setosa" and sends it to one reduce task, etc.  Each reduce task takes its input, sums the sums and lengths, and emits a resulting mean.

Hopefully these examples start give an impression of the types of things that can be done with MapReduce and how it can be done in datadr.

Remember that this MapReduce interface works on any backend, specifically RHIPE.  Those familiar with RHIPE will notice that the interface is nearly identical to that of RHIPE, but we have made some changes to make it more general.

<!-- ```{r mean_map2}
map <- expression({
  for(i in seq_along(map.keys)) {
    k <- map.keys[[i]]
    v <- map.values[[i]]

    tmp <- by(v, v$Species, function(x) {
      curSpecies <- as.character(x$Species[1])
      collect(
        curSpecies,
        data.frame(tot=sum(x$Petal.Length), n=nrow(x)))
    })
  }
}) -->

## Other Options

The examples we have seen have illustrated basic functionality of MapReduce in datadr.  There are additional options that provide fine-tuned control over some of the aspects of the MapReduce execution.

### The `setup` expression

In addition to `map` and `reduce`, another expression that can be provided to `mrExec()` is `setup`.  This expression is executed prior to any map or reduce tasks, and is typically used to load a required library, etc.  Depending on the backend, your `map` and `reduce` expression code may be executed on multiple nodes of a cluster, and these remote R sessions need to have all of the data and packages available to do the correct computation on your data.

For example, suppose in the mean by species example that we wanted to use the `plyr` package to compute the mean by species inside each map task.  Then we could specify:

```{r mean_setup}
# example of a setup expression
setup <- expression({
  suppressMessages(library(plyr))
})
```

It is a good practice to wrap calls to `library()` with `suppressMessages()` because some backends such as RHIPE interpret console output as an error.  Now we could change our map expression to something like this:

```{r mean_map2}
# alternative to meanMap using plyr
meanMap2 <- expression({
  v <- do.call(rbind, map.values)
  dlply(v, .(Species), function(x) {
    collect(
      as.character(x$Species[1]),
      cbind(tot = sum(x$Petal.Length), n = nrow(x)))
  })
})
```

We can execute it with:

```{r mean_exec2, message=FALSE}
meanRes <- mrExec(irisRR,
  setup = setup,
  map = meanMap2,
  reduce = meanReduce
)
```

### The `params` argument

If your `map` and/or `reduce` expressions rely on data in your local environment, you need to specify these in a named list as the `params` argument to `mrExec()`.  The reason for this is that the `map` and `reduce` will be executed on remote machines and any data that they rely on has to be packaged up and shipped to the nodes.  Note that when using `r rdl("divide()")` and `r rdl("recombine()")`, any functions you supply are searched to see if they reference local data objects and they are added to `params` automatically for the MapReduce calls done inside those functions, so you do not need to worry about it in those cases.

Suppose, for example, in our mean calculation, we want to convert the petal length measurement from centimeters to millimeters, using a conversion factor `cm2mm = 10` that is an object available in the global environment.  Of course this is a silly example because we could simply multiply the result by 10 in the reduce without passing the object, and also because we could do the conversion after reading the result back in.  More realistic cases will surely arise in your actual analyses, but for now, we use this example just to illustrate:

```{r params_example, message=FALSE, results="hide"}
cm2mm <- 10

meanMap3 <- expression({
  v <- do.call(rbind, map.values)
  dlply(v, .(Species), function(x) {
    collect(
      as.character(x$Species[1]),
      cbind(tot = sum(x$Petal.Length) * cm2mm, n = nrow(x)))
  })
})

meanRes <- mrExec(irisRR,
  setup = setup,
  map = meanMap3,
  reduce = meanReduce,
  params = list(cm2mm = cm2mm)
)
```

### The `control` argument

The `control` argument to `r rdl("mrExec()")` provides a way to specify backend-specific parameters that determine how various aspects of the backend will operate (such as number of map and reduce tasks, buffer sizes, number of cores to use, etc.).  As these depend on the backend being used, we will discuss `control` individually for each backend in the [Store/Compute Backends](#backend-choices) section.

Note that the `control` argument is available in `r rdl("divide()")` and `r rdl("recombine()")` as well.

### The `output` argument

The output argument allows you to specify where and how the output will be stored.  This is to be a "kvConnection" object, described in the [Store/Compute Backends](#backend-choices) section for each implemented backend.

If `output=NULL` (the default), then an attempt will be made to read the output from whatever backend the input was in to memory.  If `output` is a different storage mechanism than `input`, a conversion will be made.

### Distributed counters: `counter()`

It is possible to increment a distributed counter inside a map or reduce expression.  This can be useful for tracking things happening inside the map and reduce processes across the entire job.  Counters can be used through the function `counter()`, which is made available to be called inside any map or reduce expression.  The counter takes 3 arguments:

```r
counter(group, name, value)
```

A call to `counter()` tells the MapReduce job to add an increment of `value` to a counter identified by its `group` and `name`.

For example, let's add a counter to our example job:

```{r counter_example, message=FALSE}
meanMap4 <- expression({
  counter("counterTest", "mapValuesProcessed", length(map.values))

  v <- do.call(rbind, map.values)
  dlply(v, .(Species), function(x) {
    collect(
      as.character(x$Species[1]),
      cbind(tot = sum(x$Petal.Length) * cm2mm, n = nrow(x)))
  })
})

meanRes <- mrExec(irisRR,
  setup = setup,
  map = meanMap4,
  reduce = meanReduce,
  params = list(cm2mm = cm2mm)
)
```

We added a counter to the map expression that increments the distributed counter in group `"counterTest"` with the name `"mapValuesProcessed"`.  As map tasks running in parallel are provided new data, the length of `map.values` is added to this distributed counter.  Counters are stored as an attribute of the result, and we can look at the counters with the following:

```{r counter_res}
counters(meanRes)
```

The result is what we expect -- there were 4 input key-value pairs processed by the map.



<!-- #### `status()`

Mainly used for RHIPE. -->

<!--
The output from the map function is processed before being sent to the reduce function, grouping the key-value pairs by key. There is one reducer for each unique key passed from the map. Each group is processed by the reducer by iterating through all the values in the group. The reduce expression is composed of three parts, "pre", "reduce", and "post". For each unique key, the "pre" and "post" expressions are executed before and after the iteration through the group elements. The "pre" expression is useful for initializing variables and the "post" expression is useful for collating results or preparing them for output.
-->
