<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>datadr R function reference</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    
    <link href="assets/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="assets/custom/custom.css" rel="stylesheet">
    <!-- font-awesome -->
    <link href="assets/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    
    <!-- prism -->
    <link href="assets/prism/prism.css" rel="stylesheet">
    <link href="assets/prism/prism.r.css" rel="stylesheet">
    <script type='text/javascript' src='assets/prism/prism.js'></script>
    <script type='text/javascript' src='assets/prism/prism.r.js'></script>
    
    
    
    
    
    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="js/html5shiv.js"></script>
    <![endif]-->
    
    <link href='http://fonts.googleapis.com/css?family=Lato' rel='stylesheet' type='text/css'>
    <!-- <link href='http://fonts.googleapis.com/css?family=Lustria' rel='stylesheet' type='text/css'> -->
    <link href='http://fonts.googleapis.com/css?family=Bitter' rel='stylesheet' type='text/css'>
    

    <!-- Fav and touch icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="ico/apple-touch-icon-114-precomposed.png">
      <link rel="apple-touch-icon-precomposed" sizes="72x72" href="ico/apple-touch-icon-72-precomposed.png">
                    <link rel="apple-touch-icon-precomposed" href="ico/apple-touch-icon-57-precomposed.png">
                                   <!-- <link rel="shortcut icon" href="ico/favicon.png"> -->
  </head>

  <body>

    <div class="container-narrow">

      <div class="masthead">
        <ul class="nav nav-pills pull-right">
           <li class=''><a href='index.html'>Docs</a></li><li class='active'><a href='functionref.html'>Function Ref</a></li><li><a href='https://github.com/tesseradata/datadr'>Github <i class='fa fa-github'></i></a></li>
        </ul>
        <p class="myHeader">datadr R function reference</p>
      </div>

      <hr>

<div class="container-fluid">
   <div class="row-fluid">
   
   <div class="col-md-3 well">
   <ul class = "nav nav-list" id="toc">
   <li class='nav-header'>Contents</li> <li class="active">
   <a target="_self" class="nav-not-header" href="#packagemain">Package Info</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#adddata">addData</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#addtransform">addTransform</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#adult">adult</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#applytransform">applyTransform</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#asdataframeddf">as.data.frame.ddf</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#aslistddo">as.list.ddo</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#bsv">bsv</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#charfilehash">charFileHash</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#combcollect">combCollect</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#combddo">combDdo</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#combmean">combMean</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#combmeancoef">combMeanCoef</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#combrbind">combRbind</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#conddiv">condDiv</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#convert">convert</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#datadr">datadr</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#ddf-class">ddf-class</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#ddf">ddf</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#kvexample">kvExample</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#ddo">ddo</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#setattributes">setAttributes</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#digestfilehash">digestFileHash</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#divide">divide</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#dfsplit">dfSplit</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#draggregate">drAggregate</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drblb">drBLB</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drfilter">drFilter</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drgetglobals">drGetGlobals</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drglm">drGLM</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drhexbin">drHexbin</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drjoin">drJoin</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drlapply">drLapply</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drquantile">drQuantile</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drreadtable">drRead.table</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drsample">drSample</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#drsubset">drSubset</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#flatten">flatten</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#getbsv">getBsv</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#getbsvs">getBsvs</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#getsplitvar">getSplitVar</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#getsplitvars">getSplitVars</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#hdfsconn">hdfsConn</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#kvapply">kvApply</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#kvpairs">kvPairs</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#localdiskconn">localDiskConn</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#localdiskcontrol">localDiskControl</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#makeextractable">makeExtractable</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#mrexec">mrExec</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#tabulatemap">tabulateMap</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#printddo">print.ddo</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#printkvpair">print.kvPair</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#printkvvalue">print.kvValue</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#readhdfstextfile">readHDFStextFile</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#readtextfilebychunk">readTextFileByChunk</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#recombine">recombine</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#removedata">removeData</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#rhipecontrol">rhipeControl</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#rrdiv">rrDiv</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#setuptransformenv">setupTransformEnv</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#sparkcontrol">sparkControl</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#sparkdataconn">sparkDataConn</a>
</li>
<li class="">
   <a target="_self" class="nav-not-header" href="#updateattributes">updateAttributes</a>
</li>
   </ul>
   </div>

<div class="col-md-9 tab-content" id="main-content">

<!-- packagemain -->   
<div class="tab-pane active" id="packagemain">

<h3>Divide and Recombine for Large, Complex Data</h3>

<p><strong>Author:</strong> Ryan Hafen</p>
<p><strong>Version:</strong> 0.7.3.1</p>
<p><strong>Date:</strong> 2014-07-21</p>
<p><strong>License:</strong> BSD_3_clause + file LICENSE</p>

<h4>Description</h4>
<p>Methods for dividing data into subsets, applying analytical
methods to the subsets, and recombining the results.  Comes with a generic
MapReduce interface as well.  Works with key-value pairs stored in memory,
on local disk, or on HDFS, in the latter case using the R and Hadoop
Integrated Programming Environment (RHIPE).  Also includes experimental
support for key-value pairs stored as Spark RDDs using the SparkR package.</p>

<h4>Depends</h4>
<p>
R (&gt;= 2.15.1),
parallel</p>

<h4>Suggests</h4>
<p>
testthat (&gt;= 0.8.1),
roxygen2 (&gt;= 4.0.0)</p>

</div>
<!-- addData -->   
<div class="tab-pane" id="adddata">

<h3 class="fref_title">Add Key-Value Pairs to a Data Connection</h3>

<h4>Description</h4>
Add key-value pairs to a data connection

<h4>Usage</h4>
<pre><code class="language-r"><div>addData(conn, data, overwrite&nbsp;=&nbsp;FALSE)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>conn</dt>
  <dd>a kvConnection object</dd>
  <dt>data</dt>
  <dd>a list of key-value pairs (list of lists where each sub-list has two elements, the key and the value)</dd>
  <dt>overwrite</dt>
  <dd>if data with the same key is already present in the data, should it be overwritten? (does not work for HDFS connections)</dd>
</dl>

  <h4>Note</h4>

  <p>This is generally not recommended for HDFS as it writes a new file each time it is called, and can result in more individual files than Hadoop likes to deal with.</p>




<h4>See also</h4>

<code><a href='#removedata'>removeData</a></code>, <code><a href='#localdiskconn'>localDiskConn</a></code>, <code><a href='#hdfsconn'>hdfsConn</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- addTransform -->   
<div class="tab-pane" id="addtransform">

<h3 class="fref_title">Add a Transformation Function to a Distributed Data Object</h3>

<h4>Description</h4>
Add a transformation function to be applied to each subset of a distributed data object

<h4>Usage</h4>
<pre><code class="language-r"><div>addTransform(obj, fn, name&nbsp;=&nbsp;NULL, params&nbsp;=&nbsp;NULL, packages&nbsp;=&nbsp;NULL)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>obj</dt>
  <dd>a distributed data object</dd>
  <dt>fn</dt>
  <dd>a function to be applied to each subset of <code>obj</code></dd>
  <dt>name</dt>
  <dd>optional name of the transformation</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to <code>obj</code> that are needed in the transformation function (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>packages</dt>
  <dd>a vector of R package names that contain functions used in <code>fn</code> (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
</dl>

  <h4>Details</h4>

  <p>When you add a transformation to a distributed data object, the transformation is not applied immediately, but is deferred until a function that kicks off a computation is done.  These include <code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>, <code><a href='#drjoin'>drJoin</a></code>, <code><a href='#drlapply'>drLapply</a></code>, <code><a href='#drfilter'>drFilter</a></code>, <code><a href='#drsample'>drSample</a></code>, <code>drSubset</code>.  When any of these are invoked on an object with a transformation attached to it, the transformation will be applied in the map phase of computation prior to any other computation.  The transformation will also be applied any time a subset of the data is requested.  Thus although the data has not been physically transformed after a call of <code>addTransform</code>, we can think of it conceptually as already being transformed.</p>

  <p>When <code>addTransform</code> is called, it is tested on a subset of the data to make sure we have all of the necessary global variables and packages loaded necessary to portably perform the transformation.</p>

  <p>It is possible to add multiple transformations to a distributed data object, in which case they are applied in the order supplied, but only one transform should be necessary.</p>



</div>


<!-- adult -->   
<div class="tab-pane" id="adult">

<h3 class="fref_title">"Census Income" Dataset</h3>

<h4>Description</h4>
"Census Income" dataset from UCI machine learning repository

<h4>Usage</h4>
<pre><code class="language-r"><div>adult</div></code></pre>

  <h4>Format</h4>

  <p>(From UCI machine learning repository)</p>

  <p><ul>
<li> age. continuous
   </li>
<li> workclass. Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked
   </li>
<li> fnlwgt. continuous
   </li>
<li> education. Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool
   education-num: continuous
   </li>
<li> marital. Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse
   </li>
<li> occupation. Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces
   </li>
<li> relationship. Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried
   </li>
<li> race. White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black
   </li>
<li> sex. Female, Male
   </li>
<li> capgain. continuous
   </li>
<li> caploss. continuous
   </li>
<li> hoursperweek. continuous
   </li>
<li> nativecountry. United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands
   </li>
<li> income. <=50K, >50K
   </li>
<li> incomebin. 0 if income<=50K, 1 if income>50K
</li>
</ul></p>


  <h4>&quot;Census Income&quot; Dataset</h4>


  <h4>References</h4>

  <p>Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository [<a href = 'http://archive.ics.uci.edu/ml'>http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p>



</div>


<!-- applyTransform -->   
<div class="tab-pane" id="applytransform">

<h3 class="fref_title">Applies the transformation function(s)</h3>

<h4>Description</h4>
This is called internally in the map phase of datadr MapReduce jobs.  It is not meant for use outside of there, but is exported for convenience.

<h4>Usage</h4>
<pre><code class="language-r"><div>applyTransform(transFns, x, env&nbsp;=&nbsp;NULL)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>transFns</dt>
  <dd>from the "transforms" attribute of a ddo object</dd>
  <dt>x</dt>
  <dd>a subset of the object</dd>
  <dt>env</dt>
  <dd>the environment in which to evaluate the function (should be instantiated from calling <code><a href='#setuptransformenv'>setupTransformEnv</a></code>) - if <code>NULL</code>, the environment will be set up for you</dd>
</dl>


</div>


<!-- as.data.frame.ddf -->   
<div class="tab-pane" id="asdataframeddf">

<h3 class="fref_title">Turn 'ddf' Object into Data Frame</h3>

<h4>Description</h4>
Rbind all the rows of a 'ddf' object into a single data frame

<h4>Usage</h4>
<pre><code class="language-r"><div>"as.data.frame"(x, row.names&nbsp;=&nbsp;NULL, optional&nbsp;=&nbsp;FALSE, keys&nbsp;=&nbsp;TRUE, splitVars&nbsp;=&nbsp;TRUE, bsvs&nbsp;=&nbsp;FALSE, ...)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a 'ddf' object</dd>
  <dt>row.names</dt>
  <dd>passed to <code>as.data.frame</code></dd>
  <dt>optional</dt>
  <dd>passed to <code>as.data.frame</code></dd>
  <dt>keys</dt>
  <dd>should the key be added as a variable in the resulting data frame? (if key is not a character, it will be replaced with a md5 hash)</dd>
  <dt>splitVars</dt>
  <dd>should the values of the splitVars be added as variables in the resulting data frame?</dd>
  <dt>bsvs</dt>
  <dd>should the values of bsvs be added as variables in the resulting data frame?</dd>
  <dt>...</dt>
  <dd>additional arguments passed to as.data.frame</dd>
</dl>


</div>


<!-- as.list.ddo -->   
<div class="tab-pane" id="aslistddo">

<h3 class="fref_title">Turn 'ddo' / 'ddf' Object into a list</h3>

<h4>Description</h4>
Turn 'ddo' / 'ddf' Object into a list

<h4>Usage</h4>
<pre><code class="language-r"><div>"as.list"(x, ...)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a 'ddo' / 'ddf' object</dd>
  <dt>...</dt>
  <dd>additional arguments passed to <code>as.list</code></dd>
</dl>


</div>


<!-- bsv -->   
<div class="tab-pane" id="bsv">

<h3 class="fref_title">Construct Between Subset Variable (BSV)</h3>

<h4>Description</h4>
Construct between subset variable (BSV)

<h4>Usage</h4>
<pre><code class="language-r"><div>bsv(val&nbsp;=&nbsp;NULL, desc&nbsp;=&nbsp;"")</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>val</dt>
  <dd>a scalar character, numeric, or date</dd>
  <dt>desc</dt>
  <dd>a character string describing the BSV</dd>
</dl>

  <h4>Details</h4>

  <p>Should be called inside the <code>bsvFn</code> argument to <code>divide</code> used for constructing a BSV list for each subset of a division.</p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#getbsvs'>getBsvs</a></code>, <code><a href='ddo-ddf-#accessors'>bsvInfo</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- charFileHash -->   
<div class="tab-pane" id="charfilehash">

<h3 class="fref_title">Character File Hash Function</h3>

<h4>Description</h4>
Function to be used to specify the file where key-value pairs get stored for local disk connections, useful when keys are scalar strings.  Should be passed as the argument <code>fileHashFn</code> to <code><a href='localDiskConn.html'>localDiskConn</a></code>.

<h4>Usage</h4>
<pre><code class="language-r"><div>charFileHash(keys, conn)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>keys</dt>
  <dd>keys to be hashed</dd>
  <dt>conn</dt>
  <dd>a "localDiskConn" object</dd>
</dl>

  <h4>Details</h4>

  <p>You shouldn't need to call this directly other than to experiment with what the output looks like or to get ideas on how to write your own custom hash.</p>




<h4>See also</h4>

<code>localDiskConn</code>, <code><a href='#digestfilehash'>digestFileHash</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- combCollect -->   
<div class="tab-pane" id="combcollect">

<h3 class="fref_title">"Collect" Recombination</h3>

<h4>Description</h4>
"Collect" recombination - simply collect the results into a local list of key-value pairs

<h4>Usage</h4>
<pre><code class="language-r"><div>combCollect(...)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>...</dt>
  <dd>...</dd>
</dl>

  <h4>Details</h4>

  <p>This is an experimental prototype.  It is to be passed as the argument <code>combine</code> to <code><a href='#recombine'>recombine</a></code>.</p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>, <code><a href='#combddo'>combDdo</a></code>, <code><a href='#combmeancoef'>combMeanCoef</a></code>, <code><a href='#combrbind'>combRbind</a></code>, <code><a href='#combmean'>combMean</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- combDdo -->   
<div class="tab-pane" id="combddo">

<h3 class="fref_title">"DDO" Recombination</h3>

<h4>Description</h4>
"DDO" recombination - simply collect the results into a "ddo" object

<h4>Usage</h4>
<pre><code class="language-r"><div>combDdo(...)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>...</dt>
  <dd>...</dd>
</dl>

  <h4>Details</h4>

  <p>This is an experimental prototype.  It is to be passed as the argument <code>combine</code> to <code><a href='#recombine'>recombine</a></code>.</p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>, <code><a href='#combcollect'>combCollect</a></code>, <code><a href='#combmeancoef'>combMeanCoef</a></code>, <code><a href='#combrbind'>combRbind</a></code>, <code><a href='#combmean'>combMean</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- combMean -->   
<div class="tab-pane" id="combmean">

<h3 class="fref_title">Mean Recombination</h3>

<h4>Description</h4>
Mean recombination

<h4>Usage</h4>
<pre><code class="language-r"><div>combMean(...)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>...</dt>
  <dd>...</dd>
</dl>

  <h4>Details</h4>

  <p>This is an experimental prototype.  It is to be passed as the argument <code>combine</code> to <code><a href='#recombine'>recombine</a></code>.</p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>, <code><a href='#combcollect'>combCollect</a></code>, <code><a href='#combddo'>combDdo</a></code>, <code><a href='#combrbind'>combRbind</a></code>, <code><a href='#combmeancoef'>combMeanCoef</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- combMeanCoef -->   
<div class="tab-pane" id="combmeancoef">

<h3 class="fref_title">Mean Coefficient Recombination</h3>

<h4>Description</h4>
Mean coefficient recombination

<h4>Usage</h4>
<pre><code class="language-r"><div>combMeanCoef(...)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>...</dt>
  <dd>...</dd>
</dl>

  <h4>Details</h4>

  <p>This is an experimental prototype.  It is to be passed as the argument <code>combine</code> to <code><a href='#recombine'>recombine</a></code>.  It expects to be dealing with named vectors including an element <code>n</code> specifying the number of rows in that subset.</p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>, <code><a href='#rrdiv'>rrDiv</a></code>, <code><a href='#combcollect'>combCollect</a></code>, <code><a href='#combddo'>combDdo</a></code>, <code><a href='#combrbind'>combRbind</a></code>, <code><a href='#combmean'>combMean</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- combRbind -->   
<div class="tab-pane" id="combrbind">

<h3 class="fref_title">"rbind" Recombination</h3>

<h4>Description</h4>
"rbind" recombination

<h4>Usage</h4>
<pre><code class="language-r"><div>combRbind(...)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>...</dt>
  <dd>...</dd>
</dl>

  <h4>Details</h4>

  <p>This is an experimental prototype.  It is to be passed as the argument <code>combine</code> to <code><a href='#recombine'>recombine</a></code>.</p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>, <code><a href='#combddo'>combDdo</a></code>, <code><a href='#combcollect'>combCollect</a></code>, <code><a href='#combmeancoef'>combMeanCoef</a></code>, <code><a href='#combmean'>combMean</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- condDiv -->   
<div class="tab-pane" id="conddiv">

<h3 class="fref_title">Conditioning Variable Division</h3>

<h4>Description</h4>
Specify conditioning variable division parameters for data division

<h4>Usage</h4>
<pre><code class="language-r"><div>condDiv(vars)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>vars</dt>
  <dd>a character string or vector of character strings specifying the variables of the input data across which to divide</dd>
</dl>

  <h4>Value</h4>

  <p>a list to be used for the "by" argument to <code><a href='#divide'>divide</a></code></p>


  <h4>Details</h4>

  <p>Currently each unique combination of values of <code>vars</code> constitutes a subset.  In the future, specifying shingles for numeric conditioning variables will be implemented.</p>


  <h4>References</h4>

  <p><ul>
<li> <a href = 'http://www.datadr.org'>http://www.datadr.org</a>
  </li>
<li> <a href = 'http://onlinelibrary.wiley.com/doi/10.1002/sta4.7/full'>Guha, S., Hafen, R., Rounds, J., Xia, J., Li, J., Xi, B., & Cleveland, W. S. (2012). Large complex data: divide and recombine (D&R) with RHIPE. <em>Stat</em>, 1(1), 53-67.</a>
</li>
</ul></p>

  <p></p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#getsplitvars'>getSplitVars</a></code>, <code><a href='#getsplitvar'>getSplitVar</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- convert -->   
<div class="tab-pane" id="convert">

<h3 class="fref_title">Convert 'ddo' / 'ddf' Objects</h3>

<h4>Description</h4>
Convert 'ddo' / 'ddf' objects between different storage backends

<h4>Usage</h4>
<pre><code class="language-r"><div>convert(from, to)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>from</dt>
  <dd>a 'ddo' or 'ddf' object</dd>
  <dt>to</dt>
  <dd>a 'kvConnection' object (created with <code><a href='#localdiskconn'>localDiskConn</a></code> or <code><a href='#hdfsconn'>hdfsConn</a></code>) or <code>NULL</code> if an in-memory 'ddo' / 'ddf' is desired</dd>
</dl>


</div>


<!-- datadr -->   
<div class="tab-pane" id="datadr">

<h3 class="fref_title">datadr.</h3>

<h4>Description</h4>
datadr.



</div>


<!-- ddf-class -->   
<div class="tab-pane" id="ddf-class">

<h3 class="fref_title">'ddf' accessors</h3>

<h4>Description</h4>
'ddf' accessors,'ddf' accessors,'ddf' accessors,'ddf' accessors,The Number of Rows/Columns of a 'ddf' object

<h4>Usage</h4>
<pre><code class="language-r"><div>nrow(x)</div>
<div>NROW(x)</div>
<div>ncol(x)</div>
<div>NCOL(x)</div>
<div>"nrow"(x)</div>
<div>"NROW"(x)</div>
<div>"ncol"(x)</div>
<div>"NCOL"(x)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a 'ddf' object</dd>
</dl>


</div>


<!-- ddf -->   
<div class="tab-pane" id="ddf">

<h3 class="fref_title">Instantiate a Distributed Data Frame ('ddf')
Instantiate a distributed data frame ('ddf')</h3>

<h4>Description</h4>
Instantiate a Distributed Data Frame ('ddf')
Instantiate a distributed data frame ('ddf')

<h4>Usage</h4>
<pre><code class="language-r"><div>ddf(conn, transFn&nbsp;=&nbsp;NULL, update&nbsp;=&nbsp;FALSE, reset&nbsp;=&nbsp;FALSE, control&nbsp;=&nbsp;NULL, verbose&nbsp;=&nbsp;TRUE)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>conn</dt>
  <dd>an object pointing to where data is or will be stored for the 'ddf' object - can be a 'kvConnection' object created from <code><a href='#localdiskconn'>localDiskConn</a></code> or <code><a href='#hdfsconn'>hdfsConn</a></code>, or a data frame or list of key-value pairs</dd>
  <dt>transFn</dt>
  <dd>transFn a function to be applied to the key-value pairs of this data prior to doing any processing, that transform the data into a data frame if it is not stored as such</dd>
  <dt>update</dt>
  <dd>should the attributes of this object be updated?  See <code><a href='#updateattributes'>updateAttributes</a></code> for more details.</dd>
  <dt>reset</dt>
  <dd>should all persistent metadata about this object be removed and the object created from scratch?  This setting does not effect data stored in the connection location.</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend should handle things if attributes are updated (most-likely parameters to <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code> and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is being done</dd>
</dl>


</div>


<!-- kvExample -->   
<div class="tab-pane" id="kvexample">

<h3 class="fref_title">Accessor Functions</h3>

<h4>Description</h4>
Accessor functions for attributes of ddo/ddf objects.  Methods also include <code>nrow</code> and <code>ncol</code> for ddf objects.,Accessor methods for 'ddo' and 'ddf' objects

<h4>Usage</h4>
<pre><code class="language-r"><div>kvExample(x)</div>
<div>bsvInfo(x)</div>
<div>counters(x)</div>
<div>splitSizeDistn(x)</div>
<div>splitRowDistn(x)</div>
<div>getKeys(x)</div>
<div>"summary"(object, ...)</div>
<div>"summary"(object, ...)</div>
<div>hasExtractableKV(x)</div>
<div>"names"(x)</div>
<div>"length"(x)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a 'ddf'/'ddo' object</dd>
  <dt>object</dt>
  <dd>a 'ddf'/'ddo' object</dd>
  <dt>...</dt>
  <dd>additional arguments</dd>
</dl>


</div>


<!-- ddo -->   
<div class="tab-pane" id="ddo">

<h3 class="fref_title">Instantiate a Distributed Data Object ('ddo')
Instantiate a distributed data object ('ddo')</h3>

<h4>Description</h4>
Instantiate a Distributed Data Object ('ddo')
Instantiate a distributed data object ('ddo')

<h4>Usage</h4>
<pre><code class="language-r"><div>ddo(conn, update&nbsp;=&nbsp;FALSE, reset&nbsp;=&nbsp;FALSE, control&nbsp;=&nbsp;NULL, verbose&nbsp;=&nbsp;TRUE)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>conn</dt>
  <dd>an object pointing to where data is or will be stored for the 'ddf' object - can be a 'kvConnection' object created from <code><a href='#localdiskconn'>localDiskConn</a></code> or <code><a href='#hdfsconn'>hdfsConn</a></code>, or a data frame or list of key-value pairs</dd>
  <dt>update</dt>
  <dd>should the attributes of this object be updated?  See <code><a href='#updateattributes'>updateAttributes</a></code> for more details.</dd>
  <dt>reset</dt>
  <dd>should all persistent metadata about this object be removed and the object created from scratch?  This setting does not effect data stored in the connection location.</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend should handle things if attributes are updated (most-likely parameters to <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code> and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is being done</dd>
</dl>


</div>


<!-- setAttributes -->   
<div class="tab-pane" id="setattributes">

<h3 class="fref_title">Managing attributes of 'ddo' or 'ddf' objects</h3>

<h4>Description</h4>
Managing attributes of 'ddo' or 'ddf' objects

<h4>Usage</h4>
<pre><code class="language-r"><div>setAttributes(obj, attrs)</div>
<div>"setAttributes"(obj, attrs)</div>
<div>"setAttributes"(obj, attrs)</div>
<div>getAttribute(obj, attrName)</div>
<div>getAttributes(obj, attrNames)</div>
<div>"getAttributes"(obj, attrNames)</div>
<div>"getAttributes"(obj, attrNames)</div>
<div>hasAttributes(obj, ...)</div>
<div>"hasAttributes"(obj, attrNames)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>attrs</dt>
  <dd>a named list of attributes to set</dd>
  <dt>obj</dt>
  <dd>'ddo' or 'ddf' object</dd>
  <dt>attrName</dt>
  <dd>name of the attribute to get</dd>
  <dt>...</dt>
  <dd>additional arguments</dd>
  <dt>attrNames</dt>
  <dd>vector of names of the attributes to get</dd>
</dl>


</div>


<!-- digestFileHash -->   
<div class="tab-pane" id="digestfilehash">

<h3 class="fref_title">Digest File Hash Function</h3>

<h4>Description</h4>
Function to be used to specify the file where key-value pairs get stored for local disk connections, useful when keys are arbitrary objects.  File names are determined using a md5 hash of the object.  This is the default argument for <code>fileHashFn</code> in <code><a href='localDiskConn.html'>localDiskConn</a></code>.

<h4>Usage</h4>
<pre><code class="language-r"><div>digestFileHash(keys, conn)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>keys</dt>
  <dd>keys to be hashed</dd>
  <dt>conn</dt>
  <dd>a "localDiskConn" object</dd>
</dl>

  <h4>Details</h4>

  <p>You shouldn't need to call this directly other than to experiment with what the output looks like or to get ideas on how to write your own custom hash.</p>




<h4>See also</h4>

<code>localDiskConn</code>, <code><a href='#charfilehash'>charFileHash</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- divide -->   
<div class="tab-pane" id="divide">

<h3 class="fref_title">Divide a Distributed Data Object</h3>

<h4>Description</h4>
Divide a ddo/ddf object into subsets based on different criteria

<h4>Usage</h4>
<pre><code class="language-r"><div>divide(data, by&nbsp;=&nbsp;NULL, spill&nbsp;=&nbsp;1e+06, filterFn&nbsp;=&nbsp;NULL, bsvFn&nbsp;=&nbsp;NULL, output&nbsp;=&nbsp;NULL, overwrite&nbsp;=&nbsp;FALSE, preTransFn&nbsp;=&nbsp;NULL, postTransFn&nbsp;=&nbsp;NULL, params&nbsp;=&nbsp;NULL, packages&nbsp;=&nbsp;NULL, control&nbsp;=&nbsp;NULL, update&nbsp;=&nbsp;FALSE, verbose&nbsp;=&nbsp;TRUE)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>data</dt>
  <dd>an object of class "ddf" or "ddo" - in the latter case, need to specify <code>preTransFn</code> to coerce each subset into a data frame</dd>
  <dt>by</dt>
  <dd>specification of how to divide the data - conditional (factor-level or shingles), random replicate, or near-exact replicate (to come) -- see details</dd>
  <dt>bsvFn</dt>
  <dd>a function to be applied to each subset that returns a list of between subset variables (BSVs)</dd>
  <dt>output</dt>
  <dd>a "kvConnection" object indicating where the output data should reside (see <code><a href='#localdiskconn'>localDiskConn</a></code>, <code><a href='#hdfsconn'>hdfsConn</a></code>).  If <code>NULL</code> (default), output will be an in-memory "ddo" object.</dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location be overwritten? (also can specify <code>overwrite = "backup"</code> to move the existing output to _bak)</dd>
  <dt>spill</dt>
  <dd>integer telling the division method how many lines of data should be collected until spilling over into a new key-value pair</dd>
  <dt>filterFn</dt>
  <dd>a function that is applied to each candidate output key-value pair to determine whether it should be (if returns <code>TRUE</code>) part of the resulting division</dd>
  <dt>preTransFn</dt>
  <dd>a transformation function (if desired) to applied to each subset prior to division - note: this is deprecated - instead use <code><a href='#addtransform'>addTransform</a></code> prior to calling divide</dd>
  <dt>postTransFn</dt>
  <dd>a transformation function (if desired) to apply to each post-division subset</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the input data that are needed in the distributed computing (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>packages</dt>
  <dd>a vector of R package names that contain functions used in <code>fn</code> (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend should handle things (most-likely parameters to <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code> and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>update</dt>
  <dd>should a MapReduce job be run to obtain additional attributes for the result data prior to returning?</dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is being done</dd>
</dl>

  <h4>Value</h4>

  <p>an object of class "ddf" if the resulting subsets are data frames.  Otherwise, an object of class "ddo".</p>


  <h4>Details</h4>

  <p>The division methods this function will support include conditioning variable division for factors (implemented -- see <code><a href='#conddiv'>condDiv</a></code>), conditioning variable division for numerical variables through shingles, random replicate (implemented -- see <code><a href='#rrdiv'>rrDiv</a></code>), and near-exact replicate.  If <code>by</code> is a vector of variable names, the data will be divided by these variables.  Alternatively, this can be specified by e.g.  <code>condDiv(c("var1", "var2"))</code>.</p>


  <h4>References</h4>

  <p><ul>
<li> <a href = 'http://www.datadr.org'>http://www.datadr.org</a>
  </li>
<li> <a href = 'http://onlinelibrary.wiley.com/doi/10.1002/sta4.7/full'>Guha, S., Hafen, R., Rounds, J., Xia, J., Li, J., Xi, B., & Cleveland, W. S. (2012). Large complex data: divide and recombine (D&R) with RHIPE. <em>Stat</em>, 1(1), 53-67.</a>
</li>
</ul></p>

  <p></p>




<h4>See also</h4>

<code><a href='#recombine'>recombine</a></code>, <code><a href='#ddo'>ddo</a></code>, <code><a href='#ddf'>ddf</a></code>, <code><a href='#conddiv'>condDiv</a></code>, <code><a href='#rrdiv'>rrDiv</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- dfSplit -->   
<div class="tab-pane" id="dfsplit">

<h3 class="fref_title">Functions used in divide()</h3>

<h4>Description</h4>
Functions used in divide()

<h4>Usage</h4>
<pre><code class="language-r"><div>dfSplit(curDF, by, seed)</div>
<div>addSplitAttrs(curSplit, bsvFn, by, postTransFn&nbsp;=&nbsp;NULL)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>curDF,seed</dt>
  <dd>arguments</dd>
  <dt>curSplit,bsvFn,by,postTransFn</dt>
  <dd>arguments</dd>
</dl>


</div>


<!-- drAggregate -->   
<div class="tab-pane" id="draggregate">

<h3 class="fref_title">Division-Agnostic Aggregation</h3>

<h4>Description</h4>
Aggregates data by cross-classifying factors, with a formula interface similar to <code>xtabs</code>

<h4>Usage</h4>
<pre><code class="language-r"><div>drAggregate(formula, data&nbsp;=&nbsp;data, by&nbsp;=&nbsp;NULL, preTransFn&nbsp;=&nbsp;NULL, maxUnique&nbsp;=&nbsp;NULL, params&nbsp;=&nbsp;NULL, packages&nbsp;=&nbsp;NULL, control&nbsp;=&nbsp;NULL)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>formula</dt>
  <dd>a <code><a href='http://www.inside-r.org/packages/cran/stats/docs/formula'>formula</a></code> object with the cross-classifying variables (separated by +) on the right hand side (or an object which can be coerced to a formula). Interactions are not allowed. On the left hand side, one may optionally give a variable name in the data representing counts; in the latter case, the columns are interpreted as corresponding to the levels of a variable. This is useful if the data have already been tabulated.</dd>
  <dt>data</dt>
  <dd>a "ddf" containing the variables in the formula <code>formula</code></dd>
  <dt>by</dt>
  <dd>an optional variable by which to split up tabulations (i.e. tabulate independently inside of each unique "by" variable value).  The only difference between specifying "by" and placing the variable in the right hand side of the formula is how the computation is done and how the result is returned.</dd>
  <dt>preTransFn</dt>
  <dd>an optional function to apply to each subset prior to performing tabulation.  The output from this function should be a data frame containing variables with names that match that of the formula provided.  Note: this is deprecated - instead use <code><a href='#addtransform'>addTransform</a></code> prior to calling divide.</dd>
  <dt>maxUnique</dt>
  <dd>the maximum number of unique combinations of variables to obtaion tabulations for.  This is meant to help against cases where a variable in the formula has a very large number of levels, to the point that it is not meaningful to tabulate and is too computationally burdonsome.  If <code>NULL</code>, it is ignored.  If a positive number, only the top and bottom <code>maxUnique</code> tabulations by frequency are kept.</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the input data that are needed in the distributed computing (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>packages</dt>
  <dd>a vector of R package names that contain functions used in <code>fn</code> (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend should handle things (most-likely parameters to <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code> and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
</dl>

  <h4>Value</h4>

  <p>a data frame of the tabulations.  When "by" is specified, it is a named list with each element corresponding to a unique "by" value, containing a data frame of tabulations.</p>


  <h4>Note</h4>

  <p>The interface is similar to <code><a href='http://www.inside-r.org/packages/cran/stats/docs/xtabs'>xtabs</a></code>, but instead of returning a full contingency table, data is returned in the form of a data frame only with rows for which there were positive counts.  This result is more similar to what is returned by <code><a href='http://www.inside-r.org/packages/cran/stats/docs/aggregate'>aggregate</a></code>.</p>




<h4>See also</h4>

<code><a href='http://www.inside-r.org/packages/cran/stats/docs/xtabs'>xtabs</a></code>, <code><a href='#updateattributes'>updateAttributes</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- drBLB -->   
<div class="tab-pane" id="drblb">

<h3 class="fref_title">Bag of Little Bootstraps Transformation Method</h3>

<h4>Description</h4>
Bag of little bootstraps transformation method

<h4>Usage</h4>
<pre><code class="language-r"><div>drBLB(x, statistic, metric, R, n)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a subset of a ddf</dd>
  <dt>statistic</dt>
  <dd>a function to apply to the subset specifying the statistic to compute.  Must have arguments 'data' and 'weights' - see details).  Must return a vector, where each element is a statistic of interest.</dd>
  <dt>metric</dt>
  <dd>a function specifying the metric to be applied to the <code>R</code> bootstrap samples of each statistic returned by <code>statistic</code>.  Expects an input vector and should output a vector.</dd>
  <dt>R</dt>
  <dd>the number of bootstrap samples</dd>
  <dt>n</dt>
  <dd>the total number of observations in the data</dd>
</dl>

  <h4>Details</h4>

  <p>It is necessary to specify <code>weights</code> as a parameter to the <code>statistic</code> function because for BLB to work efficiently, it must resample each time with a sample of size <code>n</code>.  To make this computationally possible for very large <code>n</code>, we can use <code>weights</code> (see reference for details).  Therefore, only methods with a weights option can legitimately be used here.</p>


  <h4>References</h4>

  <p>BLB paper</p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- drFilter -->   
<div class="tab-pane" id="drfilter">

<h3 class="fref_title">Filter a 'ddo' or 'ddf' Object</h3>

<h4>Description</h4>
Filter a 'ddo' or 'ddf' object

<h4>Usage</h4>
<pre><code class="language-r"><div>drFilter(x, filterFn, output&nbsp;=&nbsp;NULL, overwrite&nbsp;=&nbsp;FALSE, params&nbsp;=&nbsp;NULL, packages&nbsp;=&nbsp;NULL, control&nbsp;=&nbsp;NULL)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>an object of class 'ddo' or 'ddf'</dd>
  <dt>filterFn</dt>
  <dd>function that takes the keys and/or values and returns either <code>TRUE</code> or <code>FALSE</code> - if <code>TRUE</code>, that key-value pair will be present in the result</dd>
  <dt>output</dt>
  <dd>a "kvConnection" object indicating where the output data should reside (see <code><a href='#localdiskconn'>localDiskConn</a></code>, <code><a href='#hdfsconn'>hdfsConn</a></code>).  If <code>NULL</code> (default), output will be an in-memory "ddo" object.</dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location be overwritten? (also can specify <code>overwrite = "backup"</code> to move the existing output to _bak)</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the input data that are needed in the distributed computing (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>packages</dt>
  <dd>a vector of R package names that contain functions used in <code>fn</code> (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend should handle things (most-likely parameters to <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code> and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
</dl>

  <h4>Value</h4>

  <p>a 'ddo' or 'ddf' object</p>




<h4>See also</h4>

<code><a href='#drjoin'>drJoin</a></code>, <code><a href='#drlapply'>drLapply</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- drGetGlobals -->   
<div class="tab-pane" id="drgetglobals">

<h3 class="fref_title">Get Global Variables and Package Dependencies</h3>

<h4>Description</h4>
Get global variables and package dependencies for a function

<h4>Usage</h4>
<pre><code class="language-r"><div>drGetGlobals(f)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>f</dt>
  <dd>function</dd>
</dl>

  <h4>Value</h4>

  <p>a list of variables (named by variable) and a vector of package names</p>


  <h4>Details</h4>

  <p>This traverses the parent environments of the supplied function and finds all global variables using <code>findGlobals</code> and retrieves their values.  All package function calls are also found and a list of required packages is also returned.</p>




<h4>Author</h4>

Ryan Hafen

</div>


<!-- drGLM -->   
<div class="tab-pane" id="drglm">

<h3 class="fref_title">GLM Transformation Method</h3>

<h4>Description</h4>
GLM transformation method

<h4>Usage</h4>
<pre><code class="language-r"><div>drGLM(...)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>...</dt>
  <dd>arguments you would pass to the <code><a href='http://www.inside-r.org/packages/cran/stats/docs/glm'>glm</a></code> function</dd>
</dl>

  <h4>Details</h4>

  <p>This provides a transformation function to be called for each subset in a recombination MapReduce job that applies R's glm method and outputs the coefficients in a way that <code><a href='#combmeancoef'>combMeanCoef</a></code> knows how to deal with.  It can be applied to a ddf with <code><a href='#addtransform'>addTransform</a></code> prior to calling <code><a href='#recombine'>recombine</a></code>.</p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>, <code><a href='#rrdiv'>rrDiv</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- drHexbin -->   
<div class="tab-pane" id="drhexbin">

<h3 class="fref_title">HexBin Aggregation for Distributed Data Frames</h3>

<h4>Description</h4>
Create "hexbin" object of hexagonally binned data for a distributed data frame.  This computation is division agnostic - it does not matter how the data frame is split up.

<h4>Usage</h4>
<pre><code class="language-r"><div>drHexbin(data, xVar, yVar, xTransFn&nbsp;=&nbsp;identity, yTransFn&nbsp;=&nbsp;identity, xbins&nbsp;=&nbsp;30, shape&nbsp;=&nbsp;1, params&nbsp;=&nbsp;NULL, packages&nbsp;=&nbsp;NULL, control&nbsp;=&nbsp;NULL)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>data</dt>
  <dd>a distributed data frame</dd>
  <dt>xVar,yVar</dt>
  <dd>names of the variables to use</dd>
  <dt>xTransFn,yTransFn</dt>
  <dd>a transformation function to apply to the x and y variables prior to binning</dd>
  <dt>xbins</dt>
  <dd>the number of bins partitioning the range of xbnds</dd>
  <dt>shape</dt>
  <dd>the shape = yheight/xwidth of the plotting regions</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the input data that are needed in the distributed computing (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>packages</dt>
  <dd>a vector of R package names that contain functions used in <code>fn</code> (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend should handle things (most-likely parameters to <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code> and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
</dl>

  <h4>Value</h4>

  <p>a "hexbin" object</p>


  <h4>References</h4>

  <p>Carr, D. B. et al. (1987) Scatterplot Matrix Techniques for Large <code class = 'eq'>N</code>. <em>JASA</em> <b>83</b>, 398, 424--436.</p>




<h4>See also</h4>

<code><a href='#drquantile'>drQuantile</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- drJoin -->   
<div class="tab-pane" id="drjoin">

<h3 class="fref_title">Join Two Data Sources by Key</h3>

<h4>Description</h4>
Join two data sources by key

<h4>Usage</h4>
<pre><code class="language-r"><div>drJoin(..., output&nbsp;=&nbsp;NULL, overwrite&nbsp;=&nbsp;FALSE, postTransFn&nbsp;=&nbsp;NULL, params&nbsp;=&nbsp;NULL, packages&nbsp;=&nbsp;NULL, control&nbsp;=&nbsp;NULL)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>...</dt>
  <dd>named lists of input objects - assumed that all are of same type (all HDFS, all localDisk, all in-memory)</dd>
  <dt>output</dt>
  <dd>a "kvConnection" object indicating where the output data should reside (see <code><a href='#localdiskconn'>localDiskConn</a></code>, <code><a href='#hdfsconn'>hdfsConn</a></code>).  If <code>NULL</code> (default), output will be an in-memory "ddo" object.</dd>
  <dt>postTransFn</dt>
  <dd>an optional function to be applied to the each final key-value pair after joining</dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location be overwritten? (also can specify <code>overwrite = "backup"</code> to move the existing output to _bak)</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the input data that are needed in the distributed computing (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>packages</dt>
  <dd>a vector of R package names that contain functions used in <code>fn</code> (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend should handle things (most-likely parameters to <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code> and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
</dl>

  <h4>Value</h4>

  <p>a 'ddo' object stored in the <code>output</code> connection, where the values are named lists with names according to the names given to the input data objects, and values are the corresponding data</p>




<h4>See also</h4>

<code><a href='#drfilter'>drFilter</a></code>, <code><a href='#drlapply'>drLapply</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- drLapply -->   
<div class="tab-pane" id="drlapply">

<h3 class="fref_title">Apply a function to all key-value pairs of a ddo/ddf object</h3>

<h4>Description</h4>
Apply a function to all key-value pairs of a ddo/ddf object and get a new ddo object back, unless a different <code>combine</code> strategy is specified.

<h4>Usage</h4>
<pre><code class="language-r"><div>drLapply(X, FUN, combine&nbsp;=&nbsp;combDdo(), output&nbsp;=&nbsp;NULL, overwrite&nbsp;=&nbsp;FALSE, params&nbsp;=&nbsp;NULL, packages&nbsp;=&nbsp;NULL, control&nbsp;=&nbsp;NULL, verbose&nbsp;=&nbsp;TRUE)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>X</dt>
  <dd>an object of class "ddo" of "ddf"</dd>
  <dt>FUN</dt>
  <dd>a function to be applied to each subset</dd>
  <dt>combine</dt>
  <dd>optional method to combine the results</dd>
  <dt>output</dt>
  <dd>a "kvConnection" object indicating where the output data should reside (see <code><a href='#localdiskconn'>localDiskConn</a></code>, <code><a href='#hdfsconn'>hdfsConn</a></code>).  If <code>NULL</code> (default), output will be an in-memory "ddo" object.</dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location be overwritten? (also can specify <code>overwrite = "backup"</code> to move the existing output to _bak)</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the input data that are needed in the distributed computing (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>packages</dt>
  <dd>a vector of R package names that contain functions used in <code>fn</code> (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend should handle things (most-likely parameters to <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code> and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is being done</dd>
</dl>

  <h4>Value</h4>

  <p>depends on <code>combine</code></p>




<h4>See also</h4>

<code><a href='#recombine'>recombine</a></code>, <code><a href='#drfilter'>drFilter</a></code>, <code><a href='#drjoin'>drJoin</a></code>, <code><a href='#combddo'>combDdo</a></code>, <code><a href='#combrbind'>combRbind</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- drQuantile -->   
<div class="tab-pane" id="drquantile">

<h3 class="fref_title">Sample Quantiles for 'ddf' Objects</h3>

<h4>Description</h4>
Compute sample quantiles for 'ddf' objects

<h4>Usage</h4>
<pre><code class="language-r"><div>drQuantile(x, var, by&nbsp;=&nbsp;NULL, probs&nbsp;=&nbsp;seq(0, 1, 0.005), preTransFn&nbsp;=&nbsp;NULL, varTransFn&nbsp;=&nbsp;identity, nBins&nbsp;=&nbsp;10000, tails&nbsp;=&nbsp;100, params&nbsp;=&nbsp;NULL, packages&nbsp;=&nbsp;NULL, control&nbsp;=&nbsp;NULL, ...)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a 'ddf' object</dd>
  <dt>var</dt>
  <dd>the name of the variable to compute quantiles for</dd>
  <dt>by</dt>
  <dd>the (optional) variable by which to group quantile computations</dd>
  <dt>probs</dt>
  <dd>numeric vector of probabilities with values in [0-1]</dd>
  <dt>preTransFn</dt>
  <dd>a transformation function (if desired) to applied to each subset prior to computing quantiles (here it may be useful for adding a "by" variable that is not present) - note: this transformation should not modify <code>var</code> (use <code>varTransFn</code> for that) - also note: this is deprecated - instead use <code><a href='#addtransform'>addTransform</a></code> prior to calling divide</dd>
  <dt>varTransFn</dt>
  <dd>transformation to apply to variable prior to computing quantiles</dd>
  <dt>nBins</dt>
  <dd>how many bins should the range of the variable be split into?</dd>
  <dt>tails</dt>
  <dd>how many exact values at each tail should be retained?</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the input data that are needed in the distributed computing (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>packages</dt>
  <dd>a vector of R package names that contain functions used in <code>fn</code> (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend should handle things (most-likely parameters to <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code> and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>...</dt>
  <dd>additional arguments</dd>
</dl>

  <h4>Value</h4>

  <p>data frame of quantiles <code>q</code> and their associated f-value <code>fval</code>.  If <code>by</code> is specified, then also a variable <code>group</code>.</p>


  <h4>Details</h4>

  <p>This division-agnostic quantile calculation algorithm takes the range of the variable of interest and splits it into <code>nBins</code> bins, tabulates counts for those bins, and reconstructs a quantile approximation from them.  <code>nBins</code> should not get too large, but larger <code>nBins</code> gives more accuracy.  If <code>tails</code> is positive, the first and last <code>tails</code> ordered values are attached to the quantile estimate - this is useful for long-tailed distributions or distributions with outliers for which you would like more detail in the tails.</p>




<h4>See also</h4>

<code><a href='#updateattributes'>updateAttributes</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- drRead.table -->   
<div class="tab-pane" id="drreadtable">

<h3 class="fref_title">Data Input</h3>

<h4>Description</h4>
Reads a text file in table format and creates a distributed data frame from it, with cases corresponding to lines and variables to fields in the file.

<h4>Usage</h4>
<pre><code class="language-r"><div>drRead.table(file, header&nbsp;=&nbsp;FALSE, sep&nbsp;=&nbsp;"", quote&nbsp;=&nbsp;"\"'", dec&nbsp;=&nbsp;".", skip&nbsp;=&nbsp;0, fill&nbsp;=&nbsp;!blank.lines.skip, blank.lines.skip&nbsp;=&nbsp;TRUE, comment.char&nbsp;=&nbsp;"#", allowEscapes&nbsp;=&nbsp;FALSE, encoding&nbsp;=&nbsp;"unknown", autoColClasses&nbsp;=&nbsp;TRUE, rowsPerBlock&nbsp;=&nbsp;50000, postTransFn&nbsp;=&nbsp;identity, output&nbsp;=&nbsp;NULL, overwrite&nbsp;=&nbsp;FALSE, params&nbsp;=&nbsp;NULL, packages&nbsp;=&nbsp;NULL, control&nbsp;=&nbsp;NULL, ...)</div>
<div>drRead.csv(file, header&nbsp;=&nbsp;TRUE, sep&nbsp;=&nbsp;",", quote&nbsp;=&nbsp;"\"", dec&nbsp;=&nbsp;".", fill&nbsp;=&nbsp;TRUE, comment.char&nbsp;=&nbsp;"", ...)</div>
<div>drRead.csv2(file, header&nbsp;=&nbsp;TRUE, sep&nbsp;=&nbsp;";", quote&nbsp;=&nbsp;"\"", dec&nbsp;=&nbsp;",", fill&nbsp;=&nbsp;TRUE, comment.char&nbsp;=&nbsp;"", ...)</div>
<div>drRead.delim(file, header&nbsp;=&nbsp;TRUE, sep&nbsp;=&nbsp;"\t", quote&nbsp;=&nbsp;"\"", dec&nbsp;=&nbsp;".", fill&nbsp;=&nbsp;TRUE, comment.char&nbsp;=&nbsp;"", ...)</div>
<div>drRead.delim2(file, header&nbsp;=&nbsp;TRUE, sep&nbsp;=&nbsp;"\t", quote&nbsp;=&nbsp;"\"", dec&nbsp;=&nbsp;",", fill&nbsp;=&nbsp;TRUE, comment.char&nbsp;=&nbsp;"", ...)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>file</dt>
  <dd>input text file - can either be character string pointing to a file on local disk, or an <code><a href='#hdfsconn'>hdfsConn</a></code> object pointing to a text file on HDFS (see <code>output</code> argument below)</dd>
  <dt>header</dt>
  <dd>this and parameters other parameters below are passed to <code><a href='http://www.inside-r.org/packages/cran/utils/docs/read.table'>read.table</a></code> for each chunk being processed - see <code><a href='http://www.inside-r.org/packages/cran/utils/docs/read.table'>read.table</a></code> for more info.  Most all have defaults or appropriate defaults are set through other format-specific functions such as <code>drRead.csv</code> and <code>drRead.delim</code>.</dd>
  <dt>sep</dt>
  <dd>see <code><a href='http://www.inside-r.org/packages/cran/utils/docs/read.table'>read.table</a></code> for more info</dd>
  <dt>quote</dt>
  <dd>see <code><a href='http://www.inside-r.org/packages/cran/utils/docs/read.table'>read.table</a></code> for more info</dd>
  <dt>dec</dt>
  <dd>see <code><a href='http://www.inside-r.org/packages/cran/utils/docs/read.table'>read.table</a></code> for more info</dd>
  <dt>skip</dt>
  <dd>see <code><a href='http://www.inside-r.org/packages/cran/utils/docs/read.table'>read.table</a></code> for more info</dd>
  <dt>fill</dt>
  <dd>see <code><a href='http://www.inside-r.org/packages/cran/utils/docs/read.table'>read.table</a></code> for more info</dd>
  <dt>blank.lines.skip</dt>
  <dd>see <code><a href='http://www.inside-r.org/packages/cran/utils/docs/read.table'>read.table</a></code> for more info</dd>
  <dt>comment.char</dt>
  <dd>see <code><a href='http://www.inside-r.org/packages/cran/utils/docs/read.table'>read.table</a></code> for more info</dd>
  <dt>allowEscapes</dt>
  <dd>see <code><a href='http://www.inside-r.org/packages/cran/utils/docs/read.table'>read.table</a></code> for more info</dd>
  <dt>encoding</dt>
  <dd>see <code><a href='http://www.inside-r.org/packages/cran/utils/docs/read.table'>read.table</a></code> for more info</dd>
  <dt>...</dt>
  <dd>see <code><a href='http://www.inside-r.org/packages/cran/utils/docs/read.table'>read.table</a></code> for more info</dd>
  <dt>autoColClasses</dt>
  <dd>should column classes be determined automatically by reading in a sample?  This can sometimes be problematic because of strange ways R handles quotes in <code>read.table</code>, but keeping the default of <code>TRUE</code> is advantageous for speed.</dd>
  <dt>rowsPerBlock</dt>
  <dd>how many rows of the input file should make up a block (key-value pair) of output?</dd>
  <dt>postTransFn</dt>
  <dd>a function to be applied after a block is read in to provide any additional processingn before the block is stored</dd>
  <dt>output</dt>
  <dd>a "kvConnection" object indicating where the output data should reside.  Must be a <code><a href='#localdiskconn'>localDiskConn</a></code> object if input is a text file on local disk, or a <code><a href='#hdfsconn'>hdfsConn</a></code> object if input is a text file on HDFS.</dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location be overwritten? (also can specify <code>overwrite = "backup"</code> to move the existing output to _bak)</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the input data that are needed in <code>postTransFn</code></dd>
  <dt>packages</dt>
  <dd>a vector of R package names that contain functions used in <code>fn</code> (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend should handle things (most-likely parameters to <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code> and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
</dl>

  <h4>Value</h4>

  <p>an object of class "ddf"</p>


  <h4>Note</h4>

  <p>For local disk, the file is actually read in sequentially instead of in parallel.  This is because of possible performance issues when trying to read from the same disk in parallel.</p>

  <p>Note that if <code>skip</code> is positive and/or if <code>header</code> is <code>TRUE</code>, it will first read these in as they only occur once in the data, and we then check for these lines in each block and remove those lines if they appear.</p>

  <p>Also note that if you supply <code>"Factor"</code> column classes, they will be converted to character.</p>




<h4>Author</h4>

Ryan Hafen

</div>


<!-- drSample -->   
<div class="tab-pane" id="drsample">

<h3 class="fref_title">Take a Sample of Key-Value Pairs
Take a sample of key-value Pairs</h3>

<h4>Description</h4>
Take a Sample of Key-Value Pairs
Take a sample of key-value Pairs

<h4>Usage</h4>
<pre><code class="language-r"><div>drSample(x, fraction, output&nbsp;=&nbsp;NULL, overwrite&nbsp;=&nbsp;FALSE, control&nbsp;=&nbsp;NULL)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a 'ddo' or 'ddf' object</dd>
  <dt>fraction</dt>
  <dd>fraction of key-value pairs to keep (between 0 and 1)</dd>
  <dt>output</dt>
  <dd>a "kvConnection" object indicating where the output data should reside (see <code><a href='#localdiskconn'>localDiskConn</a></code>, <code><a href='#hdfsconn'>hdfsConn</a></code>).  If <code>NULL</code> (default), output will be an in-memory "ddo" object.</dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location be overwritten? (also can specify <code>overwrite = "backup"</code> to move the existing output to _bak)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend should handle things (most-likely parameters to <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code> and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
</dl>


</div>


<!-- drSubset -->   
<div class="tab-pane" id="drsubset">

<h3 class="fref_title">Subsetting Distributed Data Frames</h3>

<h4>Description</h4>
Return a subset of a "ddf" object to memory

<h4>Usage</h4>
<pre><code class="language-r"><div>drSubset(data, subset&nbsp;=&nbsp;NULL, select&nbsp;=&nbsp;NULL, drop&nbsp;=&nbsp;FALSE, preTransFn&nbsp;=&nbsp;NULL, maxRows&nbsp;=&nbsp;5e+05, params&nbsp;=&nbsp;NULL, packages&nbsp;=&nbsp;NULL, control&nbsp;=&nbsp;NULL, verbose&nbsp;=&nbsp;TRUE)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>data</dt>
  <dd>object to be subsetted -- an object of class "ddf" or "ddo" - in the latter case, need to specify <code>preTransFn</code> to coerce each subset into a data frame</dd>
  <dt>subset</dt>
  <dd>logical expression indicating elements or rows to keep: missing values are taken as false</dd>
  <dt>select</dt>
  <dd>expression, indicating columns to select from a data frame</dd>
  <dt>drop</dt>
  <dd>passed on to [ indexing operator</dd>
  <dt>preTransFn</dt>
  <dd>a transformation function (if desired) to applied to each subset prior to division - note: this is deprecated - instead use <code><a href='#addtransform'>addTransform</a></code> prior to calling divide</dd>
  <dt>maxRows</dt>
  <dd>the maximum number of rows to return</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the input data that are needed in the distributed computing (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>packages</dt>
  <dd>a vector of R package names that contain functions used in <code>fn</code> (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend should handle things (most-likely parameters to <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code> and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is being done</dd>
</dl>

  <h4>Value</h4>

  <p>data frame</p>




<h4>Author</h4>

Ryan Hafen

</div>


<!-- flatten -->   
<div class="tab-pane" id="flatten">

<h3 class="fref_title">"Flatten" a ddf Subset</h3>

<h4>Description</h4>
Add split variables and BSVs (if any) as columns to a subset of a ddf.

<h4>Usage</h4>
<pre><code class="language-r"><div>flatten(x)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a value of a key-value pair</dd>
</dl>



<h4>See also</h4>

<code><a href='#getsplitvars'>getSplitVars</a></code>, <code><a href='#getbsvs'>getBsvs</a></code>

</div>


<!-- getBsv -->   
<div class="tab-pane" id="getbsv">

<h3 class="fref_title">Get Between Subset Variable</h3>

<h4>Description</h4>
For a given key-value pair, get a BSV variable value by name (if present)

<h4>Usage</h4>
<pre><code class="language-r"><div>getBsv(x, name)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a key-value pair or a value</dd>
  <dt>name</dt>
  <dd>the name of the BSV to get</dd>
</dl>


</div>


<!-- getBsvs -->   
<div class="tab-pane" id="getbsvs">

<h3 class="fref_title">Get Between Subset Variables</h3>

<h4>Description</h4>
For a given key-value pair, exract all BSVs

<h4>Usage</h4>
<pre><code class="language-r"><div>getBsvs(x)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a key-value pair or a value</dd>
</dl>


</div>


<!-- getSplitVar -->   
<div class="tab-pane" id="getsplitvar">

<h3 class="fref_title">Extract "Split" Variable</h3>

<h4>Description</h4>
For a given key-value pair or value, get a split variable value by name, if present (split variables are variables that define how the data was divided).

<h4>Usage</h4>
<pre><code class="language-r"><div>getSplitVar(x, name)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a key-value pair or a value</dd>
  <dt>name</dt>
  <dd>the name of the split variable to get</dd>
</dl>


</div>


<!-- getSplitVars -->   
<div class="tab-pane" id="getsplitvars">

<h3 class="fref_title">Extract "Split" Variables</h3>

<h4>Description</h4>
For a given k/v pair or value, exract all split variables (split variables are variables that define how the data was divided).

<h4>Usage</h4>
<pre><code class="language-r"><div>getSplitVars(x)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>a key-value pair or a value</dd>
</dl>


</div>


<!-- hdfsConn -->   
<div class="tab-pane" id="hdfsconn">

<h3 class="fref_title">Connect to Data Source on HDFS</h3>

<h4>Description</h4>
Connect to a data source on HDFS

<h4>Usage</h4>
<pre><code class="language-r"><div>hdfsConn(loc, type&nbsp;=&nbsp;"sequence", autoYes&nbsp;=&nbsp;FALSE, reset&nbsp;=&nbsp;FALSE, verbose&nbsp;=&nbsp;TRUE)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>loc</dt>
  <dd>location on HDFS for the data source</dd>
  <dt>type</dt>
  <dd>the type of data ("map", "sequence", "text")</dd>
  <dt>autoYes</dt>
  <dd>automatically answer "yes" to questions about creating a path on HDFS</dd>
  <dt>reset</dt>
  <dd>should existing metadata for this object be overwritten?</dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is being done</dd>
</dl>

  <h4>Value</h4>

  <p>a "kvConnection" object of class "hdfsConn"</p>


  <h4>Details</h4>

  <p>This simply creates a "connection" to a directory on HDFS (which need not have data in it).  To actually do things with this data, see <code><a href='#ddo'>ddo</a></code>, etc.</p>




<h4>See also</h4>

<code>addData</code>, <code><a href='#ddo'>ddo</a></code>, <code><a href='#ddf'>ddf</a></code>, <code><a href='#localdiskconn'>localDiskConn</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- kvApply -->   
<div class="tab-pane" id="kvapply">

<h3 class="fref_title">Apply Function to Key-Value Pair</h3>

<h4>Description</h4>
Apply a function to a single key-value pair - not a traditional R "apply" function.

<h4>Usage</h4>
<pre><code class="language-r"><div>kvApply(fn, kvPair, returnKV&nbsp;=&nbsp;FALSE)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>fn</dt>
  <dd>a function</dd>
  <dt>kvPair</dt>
  <dd>a key-value pair (a list with 2 elements)</dd>
  <dt>returnKV</dt>
  <dd>should the key and value be returned?</dd>
</dl>

  <h4>Details</h4>

  <p>Determines how a function should be applied to a key-value pair and then applies it: if the function has two formals, it applies the function giving it the key and the value as the arguments; if the function has one formal, it applies the function giving it just the value.  This provides flexibility and simplicity for when a function is only meant to be applied to the value, but still allows keys to be used if desired.</p>



</div>


<!-- kvPairs -->   
<div class="tab-pane" id="kvpairs">

<h3 class="fref_title">Manually Specify Key-Value Pairs</h3>

<h4>Description</h4>
Manually specify key-value pairs

<h4>Usage</h4>
<pre><code class="language-r"><div>kvPairs(...)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>...</dt>
  <dd>key-value pairs (lists with two elements)</dd>
</dl>

  <h4>Value</h4>

  <p>a list of objects of class "kvPair"</p>




<h4>Author</h4>

Ryan Hafen

</div>


<!-- localDiskConn -->   
<div class="tab-pane" id="localdiskconn">

<h3 class="fref_title">Connect to Data Source on Local Disk</h3>

<h4>Description</h4>
Connect to a data source on local disk

<h4>Usage</h4>
<pre><code class="language-r"><div>localDiskConn(loc, nBins&nbsp;=&nbsp;0, fileHashFn&nbsp;=&nbsp;NULL, autoYes&nbsp;=&nbsp;FALSE, reset&nbsp;=&nbsp;FALSE, verbose&nbsp;=&nbsp;TRUE)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>loc</dt>
  <dd>location on local disk for the data source</dd>
  <dt>nBins</dt>
  <dd>number of bins (subdirectories) to put data files into - if anticipating a large number of k/v pairs, it is a good idea to set this to something bigger than 0</dd>
  <dt>fileHashFn</dt>
  <dd>an optional function that operates on each key-value pair to determine the subdirectory structure for where the data should be stored for that subset, or can be specified "asis" when keys are scalar strings</dd>
  <dt>autoYes</dt>
  <dd>automatically answer "yes" to questions about creating a path on local disk</dd>
  <dt>reset</dt>
  <dd>should existing metadata for this object be overwritten?</dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is being done</dd>
</dl>

  <h4>Value</h4>

  <p>a "kvConnection" object of class "localDiskConn"</p>


  <h4>Details</h4>

  <p>This simply creates a "connection" to a directory on local disk (which need not have data in it).  To actually do things with this connection, see <code><a href='#ddo'>ddo</a></code>, etc.  Typically, you should just use <code>loc</code> to specify where the data is or where you would like data for this connection to be stored.  Metadata for the object is also stored in this directory.</p>




<h4>See also</h4>

<code>addData</code>, <code><a href='#ddo'>ddo</a></code>, <code><a href='#ddf'>ddf</a></code>, <code><a href='#localdiskconn'>localDiskConn</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- localDiskControl -->   
<div class="tab-pane" id="localdiskcontrol">

<h3 class="fref_title">Specify Control Parameters for MapReduce on a Local Disk Connection</h3>

<h4>Description</h4>
Specify control parameters for a MapReduce on a local disk connection.  Currently the parameters include:

<h4>Usage</h4>
<pre><code class="language-r"><div>localDiskControl(cluster&nbsp;=&nbsp;NULL, map_buff_size_bytes&nbsp;=&nbsp;10485760, reduce_buff_size_bytes&nbsp;=&nbsp;10485760, map_temp_buff_size_bytes&nbsp;=&nbsp;10485760)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>cluster</dt>
  <dd>a "cluster" object obtained from <code><a href='http://www.inside-r.org/packages/cran/parallel/docs/makeCluster'>makeCluster</a></code> to allow for parallel processing</dd>
  <dt>map_buff_size_bytes</dt>
  <dd>determines how much data should be sent to each map task</dd>
  <dt>reduce_buff_size_bytes</dt>
  <dd>determines how much data should be sent to each reduce task</dd>
  <dt>map_temp_buff_size_bytes</dt>
  <dd>determines the size of chunks written to disk in between the map and reduce</dd>
</dl>

  <h4>Note</h4>

  <p>If you have data on a shared drive that multiple nodes can access or a high performance shared file system like Lustre, you can run a local disk MapReduce job on multiple nodes by creating a multi-node cluster with <code><a href='http://www.inside-r.org/packages/cran/parallel/docs/makeCluster'>makeCluster</a></code>.</p>

  <p>If you are using multiple cores and the input data is very small, <code>map_buff_size_bytes</code> needs to be small so that the key-value pairs will be split across cores.</p>



</div>


<!-- makeExtractable -->   
<div class="tab-pane" id="makeextractable">

<h3 class="fref_title">Take a ddo/ddf HDFS data object and turn it into a mapfile</h3>

<h4>Description</h4>
Take a ddo/ddf HDFS data object and turn it into a mapfile

<h4>Usage</h4>
<pre><code class="language-r"><div>makeExtractable(obj)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>obj</dt>
  <dd>object of class 'ddo' or 'ddf' with an HDFS connection</dd>
</dl>


</div>


<!-- mrExec -->   
<div class="tab-pane" id="mrexec">

<h3 class="fref_title">Execute a MapReduce Job</h3>

<h4>Description</h4>
Execute a MapReduce job

<h4>Usage</h4>
<pre><code class="language-r"><div>mrExec(data, setup&nbsp;=&nbsp;NULL, map&nbsp;=&nbsp;NULL, reduce&nbsp;=&nbsp;NULL, output&nbsp;=&nbsp;NULL, overwrite&nbsp;=&nbsp;FALSE, control&nbsp;=&nbsp;NULL, params&nbsp;=&nbsp;NULL, packages&nbsp;=&nbsp;NULL, verbose&nbsp;=&nbsp;TRUE)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>data</dt>
  <dd>a ddo/ddf object, or list of ddo/ddf objects</dd>
  <dt>setup</dt>
  <dd>an expression of R code (created using the R command <code>expression</code>) to be run before map and reduce</dd>
  <dt>map</dt>
  <dd>an R expression that is evaluated during the map stage. For each task, this expression is executed multiple times (see details).</dd>
  <dt>reduce</dt>
  <dd>a vector of R expressions with names pre, reduce, and post that is evaluated during the reduce stage. For example <code>reduce = expression(pre = {...}, reduce = {...}, post = {...})</code>. reduce is optional, and if not specified the map output key-value pairs will be the result. If it is not specified, then a default identity reduce is performed. Setting it to 0 will skip the reduce altogether.</dd>
  <dt>output</dt>
  <dd>a "kvConnection" object indicating where the output data should reside (see <code><a href='#localdiskconn'>localDiskConn</a></code>, <code><a href='#hdfsconn'>hdfsConn</a></code>).  If <code>NULL</code> (default), output will be an in-memory "ddo" object.</dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location be overwritten? (also can specify <code>overwrite = "backup"</code> to move the existing output to _bak)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend should handle things (most-likely parameters to <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code> and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the input data that are needed in the map or reduce phases</dd>
  <dt>packages</dt>
  <dd>a vector of R package names that contain functions used in <code>fn</code> (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is being done</dd>
</dl>

  <h4>Value</h4>

  <p>"ddo" object - to keep it simple.  It is up to the user to update or cast as "ddf" if that is the desired result.</p>




<h4>Author</h4>

Ryan Hafen

</div>


<!-- tabulateMap -->   
<div class="tab-pane" id="tabulatemap">

<h3 class="fref_title">Functions to Compute Summary Statistics in MapReduce</h3>

<h4>Description</h4>
Functions that are used to tabulate categorical variables and compute moments for numeric variables inside through the MapReduce framework.  Used in <code><a href='updateAttributes.html'>updateAttributes</a></code>.

<h4>Usage</h4>
<pre><code class="language-r"><div>tabulateMap(formula, data)</div>
<div>tabulateReduce(result, reduce.values, maxUnique&nbsp;=&nbsp;NULL)</div>
<div>calculateMoments(y, order&nbsp;=&nbsp;1, na.rm&nbsp;=&nbsp;TRUE)</div>
<div>combineMoments(m1, m2)</div>
<div>combineMultipleMoments(...)</div>
<div>moments2statistics(m)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>formula</dt>
  <dd>a formula to be used in <code><a href='http://www.inside-r.org/packages/cran/stats/docs/xtabs'>xtabs</a></code></dd>
  <dt>data</dt>
  <dd>a subset of a 'ddf' object</dd>
  <dt>maxUnique</dt>
  <dd>the maximum number of unique combinations of variables to obtaion tabulations for.  This is meant to help against cases where a variable in the formula has a very large number of levels, to the point that it is not meaningful to tabulate and is too computationally burdonsome.  If <code>NULL</code>, it is ignored.  If a positive number, only the top and bottom <code>maxUnique</code> tabulations by frequency are kept.</dd>
  <dt>result,reduce.values</dt>
  <dd>inconsequential <code>tabulateReduce</code> parameters</dd>
  <dt>y,order,na.rm</dt>
  <dd>inconsequential <code>calculateMoments</code> parameters</dd>
  <dt>m1,m2</dt>
  <dd>inconsequential <code>combineMoments</code> parameters</dd>
  <dt>...</dt>
  <dd>inconsequential parameters</dd>
  <dt>m</dt>
  <dd>inconsequential <code>moments2statistics</code> parameters</dd>
</dl>


</div>


<!-- print.ddo -->   
<div class="tab-pane" id="printddo">

<h3 class="fref_title">Print a "ddo" or "ddf" Object</h3>

<h4>Description</h4>
Print an overview of attributes of distributed data objects (ddo) or distributed data frames (ddf)

<h4>Usage</h4>
<pre><code class="language-r"><div>"print"(x, ...)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>object to be printed</dd>
  <dt>...</dt>
  <dd>additional arguments</dd>
</dl>



<h4>Author</h4>

Ryan Hafen

</div>


<!-- print.kvPair -->   
<div class="tab-pane" id="printkvpair">

<h3 class="fref_title">Print a key-value pair</h3>

<h4>Description</h4>
Print a key-value pair

<h4>Usage</h4>
<pre><code class="language-r"><div>"print"(x, ...)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>object to be printed</dd>
  <dt>...</dt>
  <dd>additional arguments</dd>
</dl>


</div>


<!-- print.kvValue -->   
<div class="tab-pane" id="printkvvalue">

<h3 class="fref_title">Print value of a key-value pair</h3>

<h4>Description</h4>
Print value of a key-value pair

<h4>Usage</h4>
<pre><code class="language-r"><div>"print"(x, ...)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>x</dt>
  <dd>object to be printed</dd>
  <dt>...</dt>
  <dd>additional arguments</dd>
</dl>


</div>


<!-- readHDFStextFile -->   
<div class="tab-pane" id="readhdfstextfile">

<h3 class="fref_title">Experimental HDFS text reader helper function</h3>

<h4>Description</h4>
Experimental helper function for reading text data on HDFS into a HDFS connection

<h4>Usage</h4>
<pre><code class="language-r"><div>readHDFStextFile(input, output&nbsp;=&nbsp;NULL, overwrite&nbsp;=&nbsp;FALSE, fn&nbsp;=&nbsp;NULL, keyFn&nbsp;=&nbsp;NULL, linesPerBlock&nbsp;=&nbsp;10000, control&nbsp;=&nbsp;NULL, update&nbsp;=&nbsp;FALSE)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>input</dt>
  <dd>a RHIPE input text handle created with <code>rhfmt</code></dd>
  <dt>output</dt>
  <dd>an output connection such as those created with <code><a href='#localdiskconn'>localDiskConn</a></code>, and <code><a href='#hdfsconn'>hdfsConn</a></code></dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location be overwritten? (also can specify <code>overwrite = "backup"</code> to move the existing output to _bak)</dd>
  <dt>fn</dt>
  <dd>function to be applied to each chunk of lines (input to function is a vector of strings)</dd>
  <dt>keyFn</dt>
  <dd>optional function to determine the value of the key for each block</dd>
  <dt>linesPerBlock</dt>
  <dd>how many lines at a time to read</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend should handle things (most-likely parameters to <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code> and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>update</dt>
  <dd>should a MapReduce job be run to obtain additional attributes for the result data prior to returning?</dd>
</dl>


</div>


<!-- readTextFileByChunk -->   
<div class="tab-pane" id="readtextfilebychunk">

<h3 class="fref_title">Experimental sequential text reader helper function</h3>

<h4>Description</h4>
Experimental helper function for reading text data sequentially from a file on disk and adding to connection using <code><a href='addData.html'>addData</a></code>

<h4>Usage</h4>
<pre><code class="language-r"><div>readTextFileByChunk(input, output, overwrite&nbsp;=&nbsp;FALSE, linesPerBlock&nbsp;=&nbsp;10000, fn&nbsp;=&nbsp;NULL, header&nbsp;=&nbsp;TRUE, skip&nbsp;=&nbsp;0, recordEndRegex&nbsp;=&nbsp;NULL, cl&nbsp;=&nbsp;NULL)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>input</dt>
  <dd>the path to an input text file</dd>
  <dt>output</dt>
  <dd>an output connection such as those created with <code><a href='#localdiskconn'>localDiskConn</a></code>, and <code><a href='#hdfsconn'>hdfsConn</a></code></dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location be overwritten? (also can specify <code>overwrite = "backup"</code> to move the existing output to _bak)</dd>
  <dt>linesPerBlock</dt>
  <dd>how many lines at a time to read</dd>
  <dt>fn</dt>
  <dd>function to be applied to each chunk of lines (see details)</dd>
  <dt>header</dt>
  <dd>does the file have a header</dd>
  <dt>skip</dt>
  <dd>number of lines to skip before reading</dd>
  <dt>recordEndRegex</dt>
  <dd>an optional regular expression that finds lines in the text file that indicate the end of a record (for multi-line records)</dd>
  <dt>cl</dt>
  <dd>a "cluster" object to be used for parallel processing, created using <code>makeCluster</code></dd>
</dl>

  <h4>Details</h4>

  <p>The function <code>fn</code> should have one argument, which should expect to receive a vector of strings, each element of which is a line in the file.  It is also possible for <code>fn</code> to take two arguments, in which case the second argument is the header line from the file (some parsing methods might need to know the header).</p>



</div>


<!-- recombine -->   
<div class="tab-pane" id="recombine">

<h3 class="fref_title">Recombine</h3>

<h4>Description</h4>
Apply an analytic recombination method to a ddo/ddf object and combine the results

<h4>Usage</h4>
<pre><code class="language-r"><div>recombine(data, combine&nbsp;=&nbsp;NULL, apply&nbsp;=&nbsp;NULL, output&nbsp;=&nbsp;NULL, overwrite&nbsp;=&nbsp;FALSE, params&nbsp;=&nbsp;NULL, packages&nbsp;=&nbsp;NULL, control&nbsp;=&nbsp;NULL, verbose&nbsp;=&nbsp;TRUE)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>data</dt>
  <dd>an object of class "ddo" of "ddf"</dd>
  <dt>apply</dt>
  <dd>a function specifying the analytic method to apply to each subset, or a pre-defined apply function (see <code><a href='#drblb'>drBLB</a></code>, <code><a href='#drglm'>drGLM</a></code>, for example)</dd>
  <dt>combine</dt>
  <dd>the method to combine the results</dd>
  <dt>output</dt>
  <dd>a "kvConnection" object indicating where the output data should reside (see <code><a href='#localdiskconn'>localDiskConn</a></code>, <code><a href='#hdfsconn'>hdfsConn</a></code>).  If <code>NULL</code> (default), output will be an in-memory "ddo" object.</dd>
  <dt>overwrite</dt>
  <dd>logical; should existing output location be overwritten? (also can specify <code>overwrite = "backup"</code> to move the existing output to _bak)</dd>
  <dt>params</dt>
  <dd>a named list of parameters external to the input data that are needed in the distributed computing (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>packages</dt>
  <dd>a vector of R package names that contain functions used in <code>fn</code> (most should be taken care of automatically such that this is rarely necessary to specify)</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend should handle things (most-likely parameters to <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code> and <code><a href='#localdiskcontrol'>localDiskControl</a></code></dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is being done</dd>
</dl>

  <h4>Value</h4>

  <p>depends on <code>combine</code></p>


  <h4>References</h4>

  <p><ul>
<li> <a href = 'http://www.datadr.org'>http://www.datadr.org</a>
  </li>
<li> <a href = 'http://onlinelibrary.wiley.com/doi/10.1002/sta4.7/full'>Guha, S., Hafen, R., Rounds, J., Xia, J., Li, J., Xi, B., & Cleveland, W. S. (2012). Large complex data: divide and recombine (D&R) with RHIPE. <em>Stat</em>, 1(1), 53-67.</a>
</li>
</ul></p>

  <p></p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#ddo'>ddo</a></code>, <code><a href='#ddf'>ddf</a></code>, <code><a href='#drglm'>drGLM</a></code>, <code><a href='#drblb'>drBLB</a></code>, <code><a href='#combmeancoef'>combMeanCoef</a></code>, <code><a href='#combmean'>combMean</a></code>, <code><a href='#combcollect'>combCollect</a></code>, <code><a href='#combrbind'>combRbind</a></code>, <code><a href='#drlapply'>drLapply</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- removeData -->   
<div class="tab-pane" id="removedata">

<h3 class="fref_title">Remove Key-Value Pairs from a Data Connection</h3>

<h4>Description</h4>
Remove key-value pairs from a data connection

<h4>Usage</h4>
<pre><code class="language-r"><div>removeData(conn, keys)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>conn</dt>
  <dd>a kvConnection object</dd>
  <dt>keys</dt>
  <dd>a list of keys indicating which k/v pairs to remove</dd>
</dl>

  <h4>Note</h4>

  <p>This is generally not recommended for HDFS as it writes a new file each time it is called, and can result in more individual files than Hadoop likes to deal with.</p>




<h4>See also</h4>

<code><a href='#removedata'>removeData</a></code>, <code><a href='#localdiskconn'>localDiskConn</a></code>, <code><a href='#hdfsconn'>hdfsConn</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- rhipeControl -->   
<div class="tab-pane" id="rhipecontrol">

<h3 class="fref_title">Specify Control Parameters for RHIPE Job</h3>

<h4>Description</h4>
Specify control parameters for a RHIPE job.  See <code>rhwatch</code> for details about each of the parameters.

<h4>Usage</h4>
<pre><code class="language-r"><div>rhipeControl(mapred&nbsp;=&nbsp;NULL, setup&nbsp;=&nbsp;NULL, combiner&nbsp;=&nbsp;FALSE, cleanup&nbsp;=&nbsp;NULL, orderby&nbsp;=&nbsp;"bytes", shared&nbsp;=&nbsp;NULL, jarfiles&nbsp;=&nbsp;NULL, zips&nbsp;=&nbsp;NULL, jobname&nbsp;=&nbsp;"")</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>mapred,setup,combiner,cleanup,orderby,shared,jarfiles,zips,jobname</dt>
  <dd>arguments to <code>rhwatch</code> in RHIPE</dd>
</dl>


</div>


<!-- rrDiv -->   
<div class="tab-pane" id="rrdiv">

<h3 class="fref_title">Random Replicate Division</h3>

<h4>Description</h4>
Specify random replicate division parameters for data division

<h4>Usage</h4>
<pre><code class="language-r"><div>rrDiv(nrows&nbsp;=&nbsp;NULL, seed&nbsp;=&nbsp;NULL)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>nrows</dt>
  <dd>number of rows each subset should have</dd>
  <dt>seed</dt>
  <dd>the random seed to use (experimental)</dd>
</dl>

  <h4>Value</h4>

  <p>a list to be used for the "by" argument to <code><a href='#divide'>divide</a></code></p>


  <h4>Details</h4>

  <p>The random replicate division method currently gets the total number of rows of the input data and divides it by <code>nrows</code> to get the number of subsets.  Then it randomly assigns each row of the input data to one of the subsets, resulting in subsets with approximately <code>nrows</code> rows.  A future implementation will make each subset have exactly <code>nrows</code> rows.</p>


  <h4>References</h4>

  <p><ul>
<li> <a href = 'http://www.datadr.org'>http://www.datadr.org</a>
  </li>
<li> <a href = 'http://onlinelibrary.wiley.com/doi/10.1002/sta4.7/full'>Guha, S., Hafen, R., Rounds, J., Xia, J., Li, J., Xi, B., & Cleveland, W. S. (2012). Large complex data: divide and recombine (D&R) with RHIPE. <em>Stat</em>, 1(1), 53-67.</a>
</li>
</ul></p>

  <p></p>




<h4>See also</h4>

<code><a href='#divide'>divide</a></code>, <code><a href='#recombine'>recombine</a></code>, <code><a href='#conddiv'>condDiv</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


<!-- setupTransformEnv -->   
<div class="tab-pane" id="setuptransformenv">

<h3 class="fref_title">Set up transformation environment</h3>

<h4>Description</h4>
This is called internally in the map phase of datadr MapReduce jobs.  It is not meant for use outside of there, but is exported for convenience.
Given an environment and collection of transformations, it populates the environment with the global variables in the transformations.

<h4>Usage</h4>
<pre><code class="language-r"><div>setupTransformEnv(transFns, env&nbsp;=&nbsp;NULL)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>transFns</dt>
  <dd>from the "transforms" attribute of a ddo object</dd>
  <dt>env</dt>
  <dd>the environment in which to evaluate the transformations</dd>
</dl>


</div>


<!-- sparkControl -->   
<div class="tab-pane" id="sparkcontrol">

<h3 class="fref_title">Specify Control Parameters for Spark Job</h3>

<h4>Description</h4>
Specify control parameters for a Spark job.  See <code>rhwatch</code> for details about each of the parameters.

<h4>Usage</h4>
<pre><code class="language-r"><div>sparkControl()</div></code></pre>


</div>


<!-- sparkDataConn -->   
<div class="tab-pane" id="sparkdataconn">

<h3 class="fref_title">Connect to Spark Data Source</h3>

<h4>Description</h4>
Connect to Spark data source (experimental).

<h4>Usage</h4>
<pre><code class="language-r"><div>sparkDataConn(data&nbsp;=&nbsp;NULL, init&nbsp;=&nbsp;list(), verbose&nbsp;=&nbsp;TRUE)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>data</dt>
  <dd>a data frame or list of key-value pairs</dd>
  <dt>init</dt>
  <dd>a named list of arguments to be passed to <code>sparkR.init</code></dd>
  <dt>verbose</dt>
  <dd>logical - print messages about what is being done</dd>
</dl>

  <h4>Note</h4>

  <p>This is currently a proof-of-concept.  It only allows in-memory data to be initialized as a Spark RDD, which is quite pointless for big data.  In the future, this will allow connections to link to data on HDFS.</p>



</div>


<!-- updateAttributes -->   
<div class="tab-pane" id="updateattributes">

<h3 class="fref_title">Update Attributes of a 'ddo' or 'ddf' Object</h3>

<h4>Description</h4>
Update attributes of a 'ddo' or 'ddf' object

<h4>Usage</h4>
<pre><code class="language-r"><div>updateAttributes(obj, control&nbsp;=&nbsp;NULL)</div></code></pre>

<h4>Arguments</h4>
<dl>
  <dt>obj</dt>
  <dd>an object of class 'ddo' or 'ddf'</dd>
  <dt>control</dt>
  <dd>parameters specifying how the backend should handle things (most-likely parameters to <code>rhwatch</code> in RHIPE) - see <code><a href='#rhipecontrol'>rhipeControl</a></code></dd>
</dl>

  <h4>Value</h4>

  <p>an object of class 'ddo' or 'ddf'</p>


  <h4>Details</h4>

  <p>This function looks for missing attributes related to a ddo or ddf (distributed data object or data frame) object and runs MapReduce to update them.  These attributes include "splitSizeDistn", "keys", "nDiv", "nRow", and "splitRowDistn".  These attributes are useful for subsequent computations that might rely on them.  The result is the input modified to reflect the updated attributes, and thus it should be used as <code>obj <- updateAttributes(obj)</code>.</p>


  <h4>References</h4>

  <p>Bennett, Janine, et al. "Numerically stable, single-pass, parallel statistics algorithms." Cluster Computing and Workshops, 2009. <em>CLUSTER'09. IEEE International Conference on.</em> IEEE, 2009.</p>




<h4>See also</h4>

<code><a href='#ddo'>ddo</a></code>, <code><a href='#ddf'>ddf</a></code>, <code><a href='#divide'>divide</a></code>


<h4>Author</h4>

Ryan Hafen

</div>


   
   <ul class="pager">
      <li><a href="#" id="previous">&larr; Previous</a></li> 
      <li><a href="#" id="next">Next &rarr;</a></li> 
   </ul>
</div>


</div>
</div>

<hr>

<div class="footer">
   <p>&copy; Tessera, 2014</p>
</div>
</div> <!-- /container -->

<script src="assets/jquery/jquery.js"></script>
<script type='text/javascript' src='assets/custom/custom.js'></script>
<script src="assets/bootstrap/js/bootstrap.min.js"></script>
<script src="assets/custom/jquery.ba-hashchange.min.js"></script>
<script src="assets/custom/nav.js"></script>

</body>
</html>